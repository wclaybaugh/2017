{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Prediction: The logarithmic utility function\n",
    "#TODO: this section doesn't make sense. What's the datatype of a(y)?\n",
    "\n",
    "The logaraithmic utility is used for probabilistic prediction when the unknown state is a future observation $y^*$. This is a subtle point, the squared error loss was used for a non-probabilistic point-prediction from the posterior predictive. But here we want to find our future observations themselves (and the entire distribution of them).\n",
    "\n",
    "The utility is defined as:\n",
    "\n",
    "$$u(a, y^*) = log(a(y^*)),$$\n",
    "\n",
    "The expected utility then is, as usual,\n",
    "\n",
    "$$\\bar{u}(a) = \\int log(a(y^*))\\, p(y^* \\vert D, M)dy^* .$$\n",
    "\n",
    "The $a$ that maximizes this utility is the posterior-predictive itself!\n",
    "\n",
    "$$\\hat{a}(y^*) = p(y^* \\vert D, M)$$\n",
    "\n",
    "The maximized utility then is:\n",
    "\n",
    "$$\\bar{u}(a) = \\int log(p(y^* \\vert D, M))\\, p(y^* \\vert D, M)\\, dy^*.$$\n",
    "\n",
    "This is just minus the entropy of the posterior predictive distribution, and the associated divergence is our old friend the KL-divergence.\n",
    "\n",
    "Our entire analysis here seems to be tautological, but is indeed at the base of model comparison. There, we started from the KL-divergence to motivate the use of log scores that went into deriving the AIC, DIC, and WAIC. But decision theory generalizes this notion to the making of any point predictions or probabilistic predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single prediction vs multiple prediction\n",
    "\n",
    "We have so far considered the notion above of predicting a single value from a future dataset. If you had such a dataset (like a test dataset) you can think of this as trying to guess the marginal predictive distribution.\n",
    "\n",
    "The theoretical derivation is the same for the joint. One can consider the joint to be derived step by step from updated posterior predictives, as the new data points \"come in\".\n",
    "\n",
    "In practice we often use n-marginal distributions for the n future points with respect to the \"old\" dataset D. Clearly the product of n-marginals is not the joint. But these methods are commonly used as:\n",
    "\n",
    "- we might not have all the  new data yet\n",
    "- the marginal predictives are easier to calculate\n",
    "- some utilities do not make a difference between the two.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions with respect to which model/distribution?\n",
    "\n",
    "So far we have considered the distribution over which we calculate the expectation of the risk to be the posterior predictive distribution of a given model $M$. But if we want to compare models, for example, using the log score as a utility, it does not make sense to do the comparision with respect to one of the distributions being evaluated.\n",
    "\n",
    "In such a case we consider a \"true distribution\" of the unseen $y$, which we of-course do not know. But in a model comparison scenario, where we are interested in comparing other distributions, we can do so without knowing the true model. This is the essential idea behind taking the difference in the KL-divergences or equivalent divergences which allow us to create a relative scale on which quantities like the DIC and WAIC can be compared.\n",
    "\n",
    "Thus we can define the *generalization* utility:\n",
    "\n",
    "$$ \\bar{u}_t(\\hat{a}) = \\int dy^* u(\\hat{a}, y^*) p_t(y^*)$$\n",
    "\n",
    "where $p_t(y^*)$ is the true predictive distribution. Notice here that we have used $\\hat{a}$ because we are already considering the action as optimal with respect to a models posterior predictive.  In other words, for example, in model comparison, we are considering the actions $log(p1)$ and $log(p2)$ to compare to each other, but with respect to the true predictive distribution in computing the overall expected utility. We have seen this method used, in conjunction with marginal posterior-predictives for single points in the definition of the WAIC.\n",
    "\n",
    "Some researchers actually try and approximate the true distribution by a **true belief** distribution for comparison and other purposes. The idea behind this distribution is that we consider a rich enough model which we believe to capture our phenomenon well after doing posterior predictive checking from it. This might be a non-parametric model like a gaussian process which we shall see soon, or an ensemble model of the type we have seen earlier and which we shall describe in a little more detail below.\n",
    "\n",
    "This is useful for calculating the difference in predictions between the distribution used and such a true belief distribution: this allows us to see how much worse we are doing. We shall not go further down the line on that, but see Vehtari and Ojanen if you are interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Model averaging\n",
    "\n",
    "Instead, let us briefly dwell on the idea of Bayesian Model averaging. We have seen this earlier, where in a very ad hoc fashion, we weighted models we were comparing by their WAIC weight, and averaged them together. These averaged models typically gave better predictions with more sensible posterior-predictive envelopes.\n",
    "\n",
    "A simple parametric model is often not enough to provide a rich enough model to serve as a true belief model, or to capture all aspects of our data to make good predictions. Thus we indulge in model averaging:\n",
    "\n",
    "$$p_{BMA}(y^* \\vert x^*, D) = \\sum_k p(y^* \\vert x^*, D, M_k) p(M_k \\vert D)$$\n",
    "\n",
    "where the averaging is with repect to weights $w_k = p(M_k \\vert D)$, the posterior probabilities of the models $M_k$, which is precisely what the Akaike weights purport to be.\n",
    "\n",
    "We can use the true belief models derived thus at places where we want to use the \"true distribution:."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where are the models?\n",
    "\n",
    "Note that you might have chosen expressive and best fit models, but if the true generating process is outside the hypothesis set of the models you are using, then you will never capture the true predictive distribution. This is called misfit or bias. Sometimes, your hypothesis set might be too expressive: this is called overfitting and the true generating process is simpler.\n",
    "\n",
    "The former is a problem for finding the true belief distribution, and is especially a problem in mechanisms like cross-validation, which we will talk about soon, where holding out data means that we can only fit a less expressive model. The latter needs amelioration by regularization with stronger priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison\n",
    "\n",
    "The key idea in model comparison is that we will sort our average utilities in some order. The exact values are not important, and may be computed with respect to some true distribution or true-belief distribution $M_{tb}$. Remember that the utility is computed (and maximized) with respect to some model $M_k$ whereas the average of the utility is computed with respect to either the true, or true belief distribution.\n",
    "\n",
    "$$\\bar{u}(M_k, \\hat{a}_k) = \\int dy^* u(\\hat{a}_k, y^*) p(y^* \\vert D, M_{tb})$$\n",
    "\n",
    "where $a_k$ is the optimal prediction under the model $M_k$. Now we compare the actions, that is, we want:\n",
    "\n",
    "$$\\hat{M} = \\arg\\max_k \\bar{u}(M_k, \\hat{a}_k)$$\n",
    "\n",
    "There is no-calibration of these actions. However, calculating the standard error of the difference can be used to see if the difference is significant, as we did with the WAIC score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the log score we first get the $M_k$ optimal prediction by\n",
    "\n",
    "$$\\bar{u}(M_k, a_k) = \\int dy^* log a_k(y^*) p(y^* \\vert D, M_{k})$$\n",
    "\n",
    "As we know, for this, $a_k = p((y^* \\vert D, M_{k})$ which we then plug in to get:\n",
    "\n",
    "$$\\bar{u}(M_k, a_k) = \\int dy^* p(y^* \\vert D, M_{k}) p(y^* \\vert D, M_{tb})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now maximize this over $M_k$. This is equivalent to minimizing the KL-divergence as it is the negative KL divergence upto a $M_k$ independent constant. This is the approach we used to develop model comparison information criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the squared loss the first step gives us $\\hat{a}_k = E_{p(y^* \\vert D,M_k)}[y^*]$. We then plug this in to get the expected utility under the true belief model\n",
    "\n",
    "$$\\bar{l}(\\hat{a_k}) = \\int dy^* \\, (\\hat{a}_k - y^*)^2 \\, p(y^* \\vert D, M_{tb}) = \\int dy^* \\, (E_{p_k}[y^*] - y^*)^2 \\, p(y^* \\vert D, M_{tb}) = Var_{p_{tb}}[y^*] + (E_{p_{tb}}[y^*] - E_{p_{k}}[y^*])^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus if we are model comparing for the squared error, we want the model whose expectation is closest to the true-belief model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk from the posterior: posterior points\n",
    "\n",
    "Now consider the problem in which $\\omega$, the unknown state of the world is some $\\theta$ posterior parameter $\\in \\Theta$. Then our utility function is of the form $u(a, \\theta)$ and our belief about the unknown state of the world is captured by the posterior distribution $p(\\theta \\vert D, M)$.\n",
    "\n",
    "The optimal prediction can be found by calculating the expected utility over the posterior:\n",
    "\n",
    "$$\\bar{u}(a) = \\int d\\theta u(a, \\theta) p(\\theta \\vert D, M)$$\n",
    "\n",
    "$$\\hat{a} = \\arg\\max_a \\bar{u}(a)$$\n",
    "\n",
    "and then the optimal utility is\n",
    "\n",
    "$$\\bar{u}(\\hat{a}) = \\int d\\theta u(\\hat{a}, \\theta) p(\\theta \\vert D, M)$$\n",
    "\n",
    "Indeed, bayesian decision theory is often formulated with respect to the posterior rather than the posterior predictive, as especially with analytically derivable utilities, it is simple to use sampling to construct these expectations over the posterior.\n",
    "\n",
    "If we identify the $\\theta$ space utility as an average over the sampling distribution:\n",
    "\n",
    "$$u(a, \\theta) = \\int u(a, y^*) p(y^* \\vert \\theta, M) dy^*$$\n",
    "\n",
    "the two approaches are equivalent and we have merely changed the order of integration.\n",
    "\n",
    "This approach can be used to give us point estimates from the posterior such as means and medians."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
