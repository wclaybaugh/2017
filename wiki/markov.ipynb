{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chain Properties\n",
    "\n",
    "##### Keywords: markov chain, MCMC, detailed balance, metropolis,  stationarity, ergodicity, transition matrix,  metropolis-hastings, irreducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "hide": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "{:.no_toc}\n",
    "* \n",
    "{: toc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov chains\n",
    "\n",
    "Markov Chains are the first example of a **stochastic process** we will see in this class.  The values in a Markov chain depend on the previous values (probabilistically), with the defining characteristic being that a given value is depenendent only on the immediate previous value.\n",
    "\n",
    "This is certainly not IID (idenpendently and identically distributed) data, which is what we have been assuming so far and will generally assume in this course, unless specified otherwise...\n",
    "\n",
    "**Definition**: A sequence of random variables taking values in a state space is called a Markov Chain if the probability of the next step only depends on the current state.\n",
    "\n",
    "Using the notation of transition probabilities to define the probability of going from state $x$ to state $y$ as $T(y \\vert x)$, we can write the \"only the most recent state counts\" criteria mathematically:\n",
    "\n",
    "$$T(x_n \\vert x_{n-1}, x_{n-1}..., x_1) = T(x_n \\vert x_{n-1})$$\n",
    "\n",
    "In the rental car example, we specified $T(y \\vert x)$ for each of the nine from_state, to_state combinations via a transition matrix. Equivalently, we wrote down a probability distribution $P(Y|X=i)$ for each state i (each row of the transition matrix was a probability distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Jargon\n",
    "We're going to define a list of properties that together force there to be a unique set of equilibrium probabilites over the states available to the markov process. That is, these conditions will assure that, like in the rental car example, we'll get the same long-run probabilites after lots of transitions, no mater how we set the starting vector.\n",
    "\n",
    "### Homogeneous\n",
    "\n",
    "A chain is homogeneous at step $t$ if the transition probabilities are independent of $t$. Thus the evolution of the Markov chain only depends on the previous state with a fixed transition matrix.\n",
    "\n",
    "Inhomogeneous markov chains might use a cycling set of transition matrices.\n",
    "\n",
    "In the rental car example we used the same transition matrix all the time, so it was homogeneous, and we'll continue to assume homogeneous unless stated otherwise.\n",
    "\n",
    "### Irreducible\n",
    "Irriducibility requires that every state is accessible in a finite number of steps from another state. That is, there are no absorbing states. In other words, one eventually gets everywhere in the chain.\n",
    "\n",
    "The classic counterexample here is two completely disjoint sets of states. For example rental car locations on opposite sides of an ocean. Cars in the Beijing loctions may reach equilirium, and the DC locations may do the same, but no one is driving a car from one to the other. So, depending on initial conditions, there is an equilibrium where 70% of the cars are in Beijing, spread among locations there and 30% are spead around DC, AND there is an equilibirum where 80% of the cars are spread around DC, and so on.\n",
    "\n",
    "### Recurrent\n",
    "States visited repeatedly are recurrent: positive recurrent if time-to-return is bounded and null recurrent otherwise. Harris recurrent if all states are visited infinitely often as $t \\to \\infty$\n",
    "\n",
    "If there are a finite number of states and Reccurent comes for free from Irreducible.\n",
    "\n",
    "### Aperiodic\n",
    "There are no deterministic loops. \n",
    "\n",
    "The classic counterexampl here is a cycle: all the stuff at state 1 goes to state 2 and so on, and all the stuff in state N goes to state 1. Although splitting the probability evenly over all nodes is an equilibrium, it's impossible to converge there. If we start with states 1 and 3 having probability 1/2 each, those 1/2s will just cycle around and never converge.\n",
    "\n",
    "![](images/mchain2.png)\n",
    "\n",
    "### Stationarity\n",
    "Stationarity and 'stationary distribution' is the technical name for \"the equilibrium distribution\". It's a distribution in that each of the states of the chain is assigned some probability (e.g. probability of the ferrari being at that airport).\n",
    "\n",
    "A stationary distribution doesn't change when multiplied by the transition matrix.\n",
    "\n",
    "That is\n",
    "\n",
    "$$sT = s$$\n",
    "\n",
    "or, at the level of individual nodes, the total probability (or number of cars) at node j matches the total number that flow to j from all nodes (including flow from j to j). The left side is the total flow into j, the right side is the amount that s specified was at j originally.\n",
    "\n",
    "$$\\sum_i s_i T_{ij} = s_j$$\n",
    "\n",
    "[Conceptually, everything that's at j will leave, some will come back if there's a j$\\rightarrow$j transition, and some new things will come in. If the new total matches the old, j's total hasn't changed. If this is true for all j, the distribution is stationary]\n",
    "\n",
    "In the case of a continuous state space we'd re-write the above as\n",
    "\n",
    "$$\\int s(x_i) T(x_{i+1} \\vert x_i) dx_i = s(x_{i+1})$$\n",
    "\n",
    "\n",
    "### Ergodicity\n",
    "Ergodic markov chains have unique stationary distributions and will converge to them.\n",
    "\n",
    "Aperiodic, irreducible, positive Harris recurrent  markov chains are ergodic, that is, in the limit of infinite (many) steps, the marginal distribution of the chain is the same. This means that if we take largely spaced about samples from a stationary markov chain, we can draw independent samples.\n",
    "\n",
    "Further, we can do $E_f[g(x)]$ via a sample from the distribution:\n",
    "\n",
    "$$\\int g(x) f(x) dx  = \\frac{1}{N} \\sum_{t=B+1}^{B+N} g(x^*_t)$$\n",
    "\n",
    "Here $x^*_t$ is whatever state the chain was in t steps after whatever initial configuration it had. Here B is called the burin (we give the chain time to forget its initial configuration). So we have this \"ergodic\" law of large numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Balance\n",
    "\n",
    "Detailed balance is an overkill condition. The above definition for stationary boiled down to \"at each node, inflow matches outflow\". Detailed balance is \"across each edge, inflow matches outflow\". That is flow from node 3 to node 7 matches flow from node 7 to node 3.\n",
    "\n",
    "$$s(x) T(y \\vert x) = s(y) T(x \\vert y)$$\n",
    "\n",
    "If detailed balance holds for a particular distribution $s(x)$, then $s$ is stationary.\n",
    "\n",
    "If to prove this, let y=x_j and sum both sides over $x$ \n",
    "$$\\int s(x_i) T(x_j \\vert x_i) dx_i = s(x_j) \\int T(x_i \\vert x_j) dx_i $$\n",
    "$$\\int s(x_i) T(x_{j} \\vert x_i) dx_i = s(x_{j})$$\n",
    "\n",
    "We've used the fact that T(x_i \\vert x_j) is a probability distribution over $x_i$ and must integrate to 1 over the $x_i$\n",
    "\n",
    "### Use\n",
    "If we're building a transition matrix and can show that it obeys detailed balance with a distribution $s(x)$ we'll be certain that $s(x)$ is a stationary distribution. In fact, it's often easier to build something that follows detailed balance, since detailed balance is much more restrictive than building a properly convergent transition matrix in general."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
