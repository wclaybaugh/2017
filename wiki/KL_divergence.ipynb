{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divergence and Deviance\n",
    "\n",
    "##### Keywords: KL-Divergence, Jensen's inequality, likelihood, log-likelihood, deviance, entropy, cross-entropy, likelihood-ratio, probabilistic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "hide": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "{:.no_toc}\n",
    "* \n",
    "{: toc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$ \\newcommand{\\kld}{\\mathcal{D}_{KL}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kullback-Leibler (KL) Divergence\n",
    "Let's dive right in with a definition and then pick it apart:\n",
    "\n",
    "$$\\kld(P, Q) = E_p[log(p/q)] = \\sum_i p_i log(\\frac{p_i}{q_i}) \\,\\,or\\, \\int_x log(\\frac{p(x)}{q(x)})dP$$\n",
    "\n",
    "To get a more interpretable form, let's break up the logarithm, and write out the integral:  \n",
    "\n",
    "$$\\kld(P, Q) = \\sum_i p_i[log(p_i)-log(q_i)]$$\n",
    "$$\\kld(P, Q) = \\sum_i p_i[-log(q_i)-(-log(p_i))]$$\n",
    "$$\\kld(P, Q) = \\sum_i p_i[S(q_i)-S(p_i)]$$\n",
    "\n",
    "We've intrduced S() as the surprise or difficulty of guessing a particular event (the negative log of the probability, per the Entropy notebook).\n",
    "\n",
    "#### Basics\n",
    "This form makes the interpretation behind KL clearest: we have some distribution P(x). We visit every point of that distribution and measure how much excess surprise the distribution Q(x) experiences, averaged across all such points. So distributions Q which assign low probablility to points that are common under p get a large KL value. KL is smallest when Q is exaclty P and the score function is zero everywhere.\n",
    "\n",
    "\n",
    "#### Non-symmetric\n",
    "Note that $\\kld(p, q)$ does not equal $\\kld(q,p)$, since the roles of Q and P are not symmetric. P comes second in the subtraction, and multiplies the subtraction. \n",
    "\n",
    "In an example, consider how the KL divergence for a standard normal distribution and a uniform distribution on [-1,1], i.e. how surprised each is by data generated by the other, in excess of the surprise inherent to the generating distribution. In one direction, the uniform distribution is EXTREMLEY surprised by data from the normal distribution. The uniform distribution thought that data outside [-1,1] was impossible, but the normal distribution produces it. In contrast, the normal distribution isn't terribly surprised to see values in the range [-1,1], but is more surprised than the uniform would be: the normal assigned lower probability to those values. Thus the KL divergences from P to Q and Q to P are definitely different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvoAAAIGCAYAAAAlTyu3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3X+cVmWdP/7XOGLiMCGKJm4o/gibUhbxtx9bkXogroU+\nMmGLNIJIEXRtK2XNpM1NSdefo0Ox/gCBbSU3tK0eVmSmmxat8NBdF/G7GjAS6iLu6kyjDDDfP3ww\n6yw/7gFu5mYOz+dfcJ1z7vM+18zc87qvuc51qtra2toCAAAUyh6VLgAAACg/QR8AAApI0AcAgAIS\n9AEAoIAEfQAAKKA9K11AZzz11FOVLgEAAHZJxx133Gbbu0XQT7Z8ATvbkiVLkiR1dXUVOX9R6Mfy\n0I/loy/LQz+Wh34sD/1YHvqxfLqiL7c2IG7qDgAAFJCgDwAABSToAwBAAQn6AABQQII+AAAUkKAP\nAAAFJOgDAEABCfoAAFBAgj4AABSQoA8AAAUk6AMAQAEJ+gAAUECCPgAAFNCelS4AAICu9YkvP9SJ\nvZ7faef/55vO2Wmvzf8yog8AQLdwwQUX5KKLLtrstiVLluSoo47Kb3/720691pQpU/Lxj3+8w/Gf\n+MQncvTRR+fiiy8uS72VZkQfAIDdziWXXJI//vGP7f9vaGjI66+/nu985zt53/veV8HKykfQBwBg\nt3PIIYd0+P9///d/50Mf+lBOO+20ClVUfqbuAABQKPX19fnkJz+ZH/3oRznzzDNzzDHH5Lzzzsui\nRYva93n31J2jjjoqCxcuzK9+9asO039+97vfZcyYMRkyZEhOPfXUfPOb30xzc3P7a1xwwQX5+te/\nnvHjx2fQoEG59tpr84Mf/CAnnXRSHnvssVx22WU5//zz86lPfSovvPBCfv7zn+fMM8/Msccem4su\nuiivvfbaTu0HQR8AgMJZtmxZbr/99kyePDn19fV5++2385d/+ZdZt27dJvvef//9+dCHPpQhQ4bk\n/vvvz4c//OH86le/yoUXXpgDDjggt9xySy699NL86Ec/ykUXXZQNGza0H/uDH/wghx12WBoaGnLO\nOe/cZNzc3JxvfOMbOe+88/KVr3wlL7/8cr74xS/mlltuyV/91V9l6tSpeeKJJ3Lbbbft1D4wdQcA\ngMJpbm7OzJkzM2jQoCTJ+vXrc8kll+S5557L0Ucf3WHfwYMHp1evXtlnn30yePDgJMltt92WQYMG\n5dZbb23f7/3vf3++8IUv5NFHH82wYcOSJDU1Nbnqqquyxx7vjJ//53/+Z1pbW/OVr3wlhx12WJLk\ntddey4wZMzJnzpyccMIJSZJf//rXefrpp3dqHxjRBwCgMKqqqpIke+65Z4dAf9BBByVJWlpaSr5G\nc3Nz/uM//iMjRozo0P6Rj3wkvXv3zu9+97v2tkMOOaQ95L/bMccc0/7v/fffP0k61LPvvvvmzTff\n7MwlbTcj+gBUVGfW87bmNpAkPXv2zNq1aze7rbW1NUmy9957J0n22muvDgF847/fPe1mS9588820\ntbW1B/R322+//dLU1NT+/83tk7wz0v/u/TbW35WM6AMA0C3sv//+Wb169Wa3vfzyy0mSvn377vB5\namtrU1VVtdmbZVevXp199913h8/RFYzoA+zGjKYD3ckJJ5yQBx98MC+88EKOOOKIDtsWLFiQP/mT\nP8nBBx+8w+epqalJXV1dHn744Xz+859vb3/88cfz5ptvZsiQITt8jq4g6AMA0C18/OMfz7333ptx\n48bloosuypFHHpnXXnstCxYsyE9+8pPcdNNNZTvXpZdemksuuSSXX355PvnJT2bVqlW5+eabc+yx\nx+bP/uzPynaenUnQB9iCUqPdRrqB7mpr719LlixJktTV1XVVOZ221157Ze7cubnjjjtyzz335JVX\nXsk+++yTD37wg5kxY0Y+8pGPlO1cw4YNy5133pk777wzl1xySfbdd998/OMfz5e+9KVUV1eX7Tw7\nU1VbW1tbpYso5amnnspxxx1XkXPvyt/s3Yl+LA/9WD6d6cvdIejv6NSdcnxPmj7kZ7tc9GN56Mfy\n6Yq+3FpONqIPsJPsCgG2MzUAUEydDvrz5s3LXXfdlZdffjl1dXWZMmVKjj322C3uf/HFF+eXv/zl\nJu2LFi1KTU3N9lULQLfjwwZAZXQq6M+fPz9Tp07NpEmTcswxx2T27NkZP358HnroofTv33+zxzz3\n3HO58MILc/bZZ3do7+r1QwEAYHdUMui3tbWlvr4+o0aNyuTJk5Mkp556akaMGJFZs2bl6quv3uSY\nN954I6tWrcpHPvKR9scIA9A9lRqRv+ELA7uoEgC2RckHZi1fvjwrV67MsGHD2tt69OiRoUOH5vHH\nH9/sMUuXLk2SHHXUUWUqEwAA2BYlR/SXLVuWJDn00EM7tPfv3z8rVqzI+vXrN1liaOnSpdlrr71y\n66235pFHHslbb72V008/PV//+tdzwAEHbFehG+9a7motLS0VPX9R6Mfy0I/lU46+LMfXoQij5S0t\nLTv9e3Jnv/4Vdz2/1e07++vgZ7s89GN56MfyqXRflgz6TU1NSbLJDbQ1NTXZsGFDWlpa0qtXrw7b\nli5dmrVr16ampiZ33HFHGhsbc+utt+Zzn/tcHnzwwey1115lvASA7TN1bmOlS6CbKPVBIOkeH8qA\n3Uun5ugnSVVV1Wa3b6597NixOfvss3PyyScneedxxUcccURGjRqVn/zkJzn33HO3udBKreVqLdny\n0I/loR/LpxyjK6W/DqXD4Y6fo5Qdr6GUnj17lqhzV+iHUipbo5/t8tCP5aEfy6er1tHfkpJBv7a2\nNknS3Nycvn37trc3Nzenurp6s0tlHnHEETniiCM6tP3pn/5p3vve97bP3wcAAHaekkF/49z8xsbG\nDvP0GxsbM2DAgM0e8+Mf/zgHHnhgTjjhhPa2tra2rF27Nn369NnBkgF2DdaH7z52ha9VEe7HoDhG\n3T+x9E7P7Lzzzxs9fee9OO1KBv0BAwakX79+WbBgQU477bQkSWtrax599NEMHTp0s8d873vfS1NT\nU37wgx9kjz3eWdjnV7/6Vd56660cf/zx5asegIp7Z/76zp8itDW7QpAHdr4LLrggTz/9dH74wx9u\nMuC8ZMmSnHvuubnvvvty0kknVabArRg2bFiGDh2aa665psvOWTLoV1VVZcKECbn22mvTu3fvDBky\nJHPmzMnrr7+esWPHJklWrFiRNWvWtK+Zf9FFF2XChAn56le/mk9+8pNZtmxZbrvttpx55pkZMmTI\nTr0ggN2JgAvsbt5+++18/etfz+zZsytdyi6vU0/GHTNmTN5+++3cd999mTlzZurq6nL33Xe3PxW3\noaEh8+fPb59//5GPfCTTp0/PnXfemUmTJqVXr14577zz8pd/+Zc770oAKCwfaICNamtrs3Dhwnz/\n+9/P+eefX+lydmmdCvpJMm7cuIwbN26z26ZNm5Zp06Z1aDvjjDNyxhln7Fh1ALs5ARego+OOOy5V\nVVW54YYbMnTo0C0+o+mll17KjTfemIULF+att97KySefnCuvvLJ9yk99fX1++ctf5vjjj88DDzyQ\nQw45JHfccUc++tGP5s4778ycOXOyePHi9O3bN1deeWUOP/zwXHPNNXn22Wdz2GGH5Zvf/GYGDRqU\n5J1p7dOnT8+Pf/zjrFy5Mj179sxJJ52UUaNGbfczpMqh5JNxAQBgV3LNNddk3bp1ufbaaze7/eWX\nX87555+f5cuX5xvf+Eauv/76vPTSS/nMZz6TV155pX2/pUuX5rnnnsudd96Zyy+/vL39a1/7Wv7f\n//t/mT59eg466KBceeWVmTRpUs4+++zcfvvtaWpqyle/+tX2/a+//vrMmTMnEyZMyD333JPLL788\nTz75ZO6+++6d1wmd0OkRfQAA2BUcfPDB+dKXvpRvfetb+cUvfpGPfvSjHbbPnDkzb731Vu65557s\nt99+SZITTzwxH/vYx3LvvfdmypQpSZJ169ZlypQp+dCHPpTknb8CJMmIESMyYcKEJMn69evzhS98\nIZ/4xCcyZsyYJMmrr76aq6++Om+88Ube+973Zs2aNbniiivyqU99qv1cv//97/Pggw/u/M7YCkEf\nAIBu57Of/Wz++Z//Od/85jc3WWXnd7/7XU466aT2kJ8k++23X0455ZQsXLiww76bWy5+45ScJO3P\nkTrmmGPa2zYuF78x6N96661JkldeeSUvvvhiXnzxxSxatCitra07dpE7yNQdAAC6nT322CN/+7d/\nm9WrV+emm27qsO2NN97o8KDXjfbff/80Nze3/3+fffbJPvvss8l+m3sg7N57773FWhYtWpSRI0fm\nz/7sz3LppZfm4Ycfznve855tuZydQtAHAKBbOuqoozJ+/Ph873vfy6JFi9rbe/fundWrV2+y/+rV\nq7PvvvuWtYY333wzF198cQ4++OD87Gc/y7/+679m9uzZOfbYY8t6nu0h6AMA0G1NmjQphxxySG6+\n+eb2tuOOOy6//e1vs2bNmva2NWvW5Mknnyz7M51efPHF/M///E8+97nP5dBDD02SbNiwIU888UTa\n2trKeq5tZY4+AADd1nve855885vfzOc+97n2trFjx2b+/PkZN25cJk6cmCSZPn169tprrw77lcPh\nhx+empqaNDQ0ZMOGDXnrrbfyD//wD3nuuedSVVVV0bAv6ANAGXjmAd3JvNHTt7htyZIlSZK6urqu\nKmeHnXzyyTnvvPPyT//0T0mSfv36Ze7cubnxxhszZcqUVFdX56STTsott9ySgw46qKznrq2tTX19\nfW644YZMnDgxffr0yfHHH5/bbrstl112WZ5//vn2VX26WlVbpf+m0AlPPfVUjjvuuIqcuzt+s++K\n9GN56MfyWbJkSa646/lKlwHtbvjCQD/bO8h7ZHnox/Lpir7cWk42Rx8AAApI0AcAgAIS9AEAoIAE\nfQAAKCBBHwAACkjQBwCAAhL0AQCggAR9AAAoIEEfAAAKSNAHAIACEvQBAKCABH0AACggQR8AAApI\n0AcAgAIS9AEAoIAEfQAAKCBBHwAACkjQBwCAAhL0AQCggAR9AAAoIEEfAAAKSNAHAIACEvQBAKCA\nBH0AACggQR8AAApI0AcAgAIS9AEAoIAEfQAAKCBBHwAACkjQBwCAAhL0AQCggAR9AAAoIEEfAAAK\nSNAHAIACEvQBAKCABH0AACggQR8AAApI0AcAgAIS9AEAoIAEfQAAKCBBHwAACkjQBwCAAhL0AQCg\ngAR9AAAoIEEfAAAKSNAHAIACEvQBAKCABH0AACggQR8AAApI0AcAgAIS9AEAoIAEfQAAKCBBHwAA\nCkjQBwCAAhL0AQCggAR9AAAoIEEfAAAKSNAHAIACEvQBAKCABH0AACggQR8AAApI0AcAgAIS9AEA\noIAEfQAAKCBBHwAACkjQBwCAAtqz0gUA0L31PPHhrW5vWTiiiyoB4N06PaI/b968DB8+PIMGDcro\n0aOzePHiTp/kjjvuyFFHHbVdBQIAANuuU0F//vz5mTp1akaOHJn6+vrU1tZm/PjxaWxsLHns888/\nn+985zs7XCgAANB5JYN+W1tb6uvrM2rUqEyePDmnn356pk+fnj59+mTWrFlbPXb9+vW56qqrst9+\n+5WtYAAAoLSSQX/58uVZuXJlhg0b1t7Wo0ePDB06NI8//vhWj505c2aam5vz2c9+dscrBQAAOq3k\nzbjLli1Lkhx66KEd2vv3758VK1Zk/fr1qa6u3uS45cuXp76+PnfddVf+/d//fYcLXbJkyQ6/xvZo\naWmp6PmLQj+Wh34sn419CbuKlpYWP9s7yHtkeejH8ql0X5YM+k1NTUmSmpqaDu01NTXZsGFDWlpa\n0qtXrw7b2tracvXVV+ecc87J8ccfX5agD0BllFpVB4BdU8mg39bWliSpqqra7PbNtf/jP/5jli9f\nnunTp+9gef+rrq6ubK+1LTZ+AqvU+YtCP5aHfiwfI1Xsanr27Olnewd5jywP/Vg+XdGXTz311Ba3\nlZyjX1tbmyRpbm7u0N7c3Jzq6upNRvpXrVqVG2+8MV/72tey9957Z926de0fFtatW5cNGzZs8wUA\nAADbpuSI/sa5+Y2NjR3m6Tc2NmbAgAGb7P/kk0+mubk5l1122SbbPvzhD2fy5Mm59NJLd6BkAACg\nlJJBf8CAAenXr18WLFiQ0047LUnS2tqaRx99NEOHDt1k/zPOOCMPPPBAh7Yf//jHuffee/PAAw/k\nwAMPLE/lAJRkfj3A7qtk0K+qqsqECRNy7bXXpnfv3hkyZEjmzJmT119/PWPHjk2SrFixImvWrMng\nwYPTp0+f9OnTp8NrbJw7dMwxx5T/CgAAgE2UDPpJMmbMmLz99tu57777MnPmzNTV1eXuu+9O//79\nkyQNDQ2ZP39+li5dulOLBQAAOqdTQT9Jxo0bl3Hjxm1227Rp0zJt2rQtHjt27Nj20X8AAGDnK7nq\nDgAA0P10ekQfALZHqRuCWxaO6KJKAHYvRvQBAKCABH0AACggU3cAqKjOrPVveg/AtjOiDwAABSTo\nAwBAAQn6AABQQII+AAAUkKAPAAAFZNUdAHZ5HroFsO2M6AMAQAEJ+gAAUECCPgAAFJCgDwAABSTo\nAwBAAVl1B6AbK7UaDQC7LyP6AABQQII+AAAUkKAPAAAFJOgDAEABCfoAAFBAgj4AABSQoA8AAAUk\n6AMAQAEJ+gAAUECejAtAt1fqCcEtC0d0USUAuw4j+gAAUECCPgAAFJCgDwAABWSOPsAurNTccwDY\nEiP6AABQQII+AAAUkKAPAAAFJOgDAEABCfoAAFBAgj4AABSQoA8AAAUk6AMAQAEJ+gAAUECCPgAA\nFJCgDwAABSToAwBAAQn6AABQQII+AAAU0J6VLgAAdraeJz681e0tC0d0USUAXceIPgAAFJCgDwAA\nBSToAwBAAZmjD1BBpeaOA8D2MqIPAAAFJOgDAEABCfoAAFBAgj4AABSQoA8AAAUk6AMAQAEJ+gAA\nUECCPgAAFJCgDwAABSToAwBAAQn6AABQQII+AAAUkKAPAAAFJOgDAEAB7VnpAgCKqueJD1e6BDqp\n1NeqZeGILqoEoHyM6AMAQAEJ+gAAUECCPgAAFJCgDwAABSToAwBAAQn6AABQQII+AAAUkKAPAAAF\nJOgDAEABeTIuwHby5FsAdmWdHtGfN29ehg8fnkGDBmX06NFZvHjxVvd/7LHHct5552Xw4MEZPnx4\nZs+enba2th0uGAAAKK1TQX/+/PmZOnVqRo4cmfr6+tTW1mb8+PFpbGzc7P6LFy/OxIkTM3DgwDQ0\nNOT888/PtGnTMmvWrLIWDwAAbF7JoN/W1pb6+vqMGjUqkydPzumnn57p06enT58+WwzuM2fOzJFH\nHpnrrrsup556aiZMmJCRI0dm7ty5Zb8AAABgUyXn6C9fvjwrV67MsGHD2tt69OiRoUOH5vHHH9/s\nMVOmTMkf//jHVFVVdThm7dq1ZSgZAAAopWTQX7ZsWZLk0EMP7dDev3//rFixIuvXr091dXWHbf36\n9Wv/9xtvvJFHHnkkDz74YCZOnLjdhS5ZsmS7j90RLS0tFT1/UejH8tCP5bOxL2FX0dLS4md7B3mP\nLA/9WD6V7suSQb+pqSlJUlNT06G9pqYmGzZsSEtLS3r16rXZY9/9l4Cjjz46n/70p3e0XgAAoBNK\nBv2NK+W8exrOu22pPUl69eqVWbNmZfXq1bntttsyevToPPjgg+nZs+c2F1pXV7fNx5TDxk9glTp/\nUejH8tCP5WOkim3RmaVUWxaO2LFz9OzpZ3sHeY8sD/1YPl3Rl0899dQWt5UM+rW1tUmS5ubm9O3b\nt729ubk51dXVm4z0v1vv3r1z8sknJ0k+8IEPZOTIkfnpT3+ac889t9PFAwAA267kqjsb5+b/36U0\nGxsbM2DAgM0es2DBgjzzzDMd2gYOHJgePXrk1Vdf3c5SAQCAzioZ9AcMGJB+/fplwYIF7W2tra15\n9NFHc8opp2z2mBkzZuTGG2/s0Pab3/wmra2tGThw4A6WDAAAlFJy6k5VVVUmTJiQa6+9Nr17986Q\nIUMyZ86cvP766xk7dmySZMWKFVmzZk0GDx6cJLn44oszceLEXHPNNTnrrLPy+9//PrfffntOPPHE\nnH766Tv1ggAAgE4E/SQZM2ZM3n777dx3332ZOXNm6urqcvfdd6d///5JkoaGhsyfPz9Lly5Nkgwb\nNiwNDQ1paGjIQw89lNra2pxzzjm5/PLLt3rzLgAAUB6dCvpJMm7cuIwbN26z26ZNm5Zp06Z1aPvo\nRz+aj370oztWHQAAsF1KztEHAAC6H0EfAAAKSNAHAIACEvQBAKCABH0AACigTq+6A7C76Xniw5Uu\nAQC2mxF9AAAoIEEfAAAKSNAHAIACMkcfAMqg1D0dLQtHdFElAO8wog8AAAUk6AMAQAEJ+gAAUECC\nPgAAFJCgDwAABSToAwBAAQn6AABQQII+AAAUkKAPAAAFJOgDAEABCfoAAFBAgj4AABSQoA8AAAUk\n6AMAQAEJ+gAAUECCPgAAFJCgDwAABSToAwBAAQn6AABQQHtWugAA2B30PPHhEnsM7JI6gN2HEX0A\nACggQR8AAApI0AcAgAIS9AEAoIDcjAvstkrfHAkA3ZcRfQAAKCBBHwAACkjQBwCAAhL0AQCggAR9\nAAAoIEEfAAAKSNAHAIACEvQBAKCABH0AACggQR8AAApI0AcAgAIS9AEAoIAEfQAAKCBBHwAACkjQ\nBwCAAhL0AQCggAR9AAAoIEEfAAAKSNAHAIACEvQBAKCABH0AACggQR8AAApI0AcAgAIS9AEAoID2\nrHQBADvDqPsnVroE2CZTn7k9eWbL2+eNnt51xQCFYEQfAAAKSNAHAIACEvQBAKCABH0AACggQR8A\nAApI0AcAgAIS9AEAoIAEfQAAKCBBHwAACkjQBwCAAhL0AQCggAR9AAAoIEEfAAAKSNAHAIACEvQB\nAKCA9qx0AQDbY9T9EytdAnSpznzPzxs9vQsqAbqLTo/oz5s3L8OHD8+gQYMyevToLF68eKv7L1q0\nKBdccEGOP/74nHbaabniiiuyevXqHS4YAAAorVNBf/78+Zk6dWpGjhyZ+vr61NbWZvz48WlsbNzs\n/i+88ELGjh2bmpqa3HTTTbnyyiuzaNGijB8/Pq2trWW9AAAAYFMlp+60tbWlvr4+o0aNyuTJk5Mk\np556akaMGJFZs2bl6quv3uSYOXPm5IADDkh9fX169OiRJDn00ENz/vnn54knnsjpp59e5ssAAADe\nrWTQX758eVauXJlhw4a1t/Xo0SNDhw7N448/vtljjjzyyBx55JHtIT9JDj/88CTJSy+9tKM1AwAA\nJZQM+suWLUvyzoj8u/Xv3z8rVqzI+vXrU11d3WHbmDFjNnmdRx55JMn/Bv5ttWTJku06bke1tLRU\n9PxFoR/LQz8CW7O7vzd4jywP/Vg+le7LknP0m5qakiQ1NTUd2mtqarJhw4b2C9iaVatW5YYbbsjR\nRx+dk08+eTtLBQAAOqtTc/STpKqqarPbt9S+0apVqzJ27Nhs2LAht9xyS8n9t6Surm67jttRGz+B\nVer8RaEfy0M/vsszlS4Adj27+3uD98jy0I/l0xV9+dRTT21xW8kR/dra2iRJc3Nzh/bm5uZUV1dv\nMtL/bs8//3z+4i/+Ik1NTbnnnntyyCGHdLZmAABgB5QM+hvn5v/fpTQbGxszYMCALR739NNPZ8yY\nMamurs7cuXPzwQ9+cMcqBQAAOq1k0B8wYED69euXBQsWtLe1trbm0UcfzSmnnLLZYxobGzNhwoT0\n7ds33/ve97b6gQAAACi/knP0q6qqMmHChFx77bXp3bt3hgwZkjlz5uT111/P2LFjkyQrVqzImjVr\nMnjw4CTJddddl6amplxzzTVZtWpVVq1a1f56Bx98cA488MCdczUAAECSTgT95J3lMt9+++3cd999\nmTlzZurq6nL33Xenf//+SZKGhobMnz8/S5cuTWtrax577LGsX78+X/7ylzd5rSuuuCLjx48v71UA\nAAAddCroJ8m4ceMybty4zW6bNm1apk2bluSdh2k9++yz5akOAADYLiXn6AMAAN2PoA8AAAUk6AMA\nQAEJ+gAAUECCPgAAFJCgDwAABSToAwBAAXV6HX2ArjTq/omVLgEAujUj+gAAUECCPgAAFJCgDwAA\nBSToAwBAAQn6AABQQII+AAAUkOU1AaAgSi1LO2/09C6qBNgVGNEHAIACEvQBAKCABH0AACggQR8A\nAApI0AcAgAIS9AEAoIAEfQAAKCBBHwAACkjQBwCAAhL0AQCggAR9AAAoIEEfAAAKSNAHAIAC2rPS\nBQC7p1H3T6x0CbDbKfVzN2/09C6qBOgKRvQBAKCABH0AACggQR8AAApI0AcAgAIS9AEAoIAEfQAA\nKCBBHwAACkjQBwCAAhL0AQCggAR9AAAoIEEfAAAKSNAHAIACEvQBAKCABH0AACggQR8AAApI0AcA\ngAIS9AEAoID2rHQBAMCuYdT9E7e6fd7o6V1UCVAORvQBAKCABH0AACggQR8AAApI0AcAgAIS9AEA\noIAEfQAAKCDLawI7Rall+gCAncuIPgAAFJCgDwAABSToAwBAAQn6AABQQII+AAAUkKAPAAAFJOgD\nAEABCfoAAFBAHpgFbDMPwwKAXZ+gDwB0SqkP+fNGT++iSoDOMHUHAAAKSNAHAIACEvQBAKCABH0A\nACggQR8AAApI0AcAgAIS9AEAoIAEfQAAKCBBHwAACkjQBwCAAhL0AQCggPasdAEAQDGMun9iyX3m\njZ7eBZUAyTaM6M+bNy/Dhw/PoEGDMnr06CxevLhTxzU1NeWMM87Iww8/vN1FAgAA26ZTQX/+/PmZ\nOnVqRo4cmfr6+tTW1mb8+PFpbGzc6nFNTU255JJL8oc//KEsxQIAAJ1TcupOW1tb6uvrM2rUqEye\nPDlJcuqpp2bEiBGZNWtWrr766s0et3DhwkydOjWvvfZaeSsGdrrO/PkdANi1lRzRX758eVauXJlh\nw4a1t/XFU0y3AAAQqklEQVTo0SNDhw7N448/vsXjJk2alIEDB+auu+4qT6UAAECnlRzRX7ZsWZLk\n0EMP7dDev3//rFixIuvXr091dfUmx82dOzcDBw7MSy+9VJZClyxZUpbX2VYtLS0VPX9R6Mfy0I9A\nd7cz37+8R5aHfiyfSvdlyRH9pqamJElNTU2H9pqammzYsKH9Av6vgQMHlqE8AABge3Rqjn6SVFVV\nbXb7ltrLra6urkvO839t/ARWqfMXhX4sjy7rx2d27ssDu6+d+f7ld0156Mfy6Yq+fOqpp7a4reSI\nfm1tbZKkubm5Q3tzc3Oqq6s3GekHAAAqr2TQ3zg3//8updnY2JgBAwbslKIAAIAdUzLoDxgwIP36\n9cuCBQva21pbW/Poo4/mlFNO2anFAQAA26fkHP2qqqpMmDAh1157bXr37p0hQ4Zkzpw5ef311zN2\n7NgkyYoVK7JmzZoMHjx4Z9cLAAB0QsmgnyRjxozJ22+/nfvuuy8zZ85MXV1d7r777vTv3z9J0tDQ\nkPnz52fp0qU7tVgAAKBzOhX0k2TcuHEZN27cZrdNmzYt06ZN2+y297///T4AAABAFys5Rx8AAOh+\nBH0AACggQR8AAApI0AcAgALq9M24AAA7atT9E7e6fd7o6V1UCRSfEX0AACggQR8AAArI1B3YDZX6\n0zkA0P0Z0QcAgAIS9AEAoIAEfQAAKCBBHwAACkjQBwCAArLqDgCwy/BALSgfI/oAAFBAgj4AABSQ\nqTtQQB6IBQAY0QcAgAIS9AEAoIAEfQAAKCBBHwAACkjQBwCAAhL0AQCggAR9AAAoIEEfAAAKSNAH\nAIACEvQBAKCA9qx0AcC2mfrM7ckzla4CANjVGdEHAIACEvQBAKCABH0AACggc/QBgG5j1P0Tt7r9\nbwZd1kWVwK7PiD4AABSQoA8AAAUk6AMAQAEJ+gAAUEBuxgUACqPUQwXnjZ7edcVAhQn6sIsptaIE\nAEBnmLoDAAAFJOgDAEABCfoAAFBAgj4AABSQoA8AAAUk6AMAQAFZXhO6mOUzAYCuIOgDALuNzgy2\neKgWRWHqDgAAFJCgDwAABSToAwBAAQn6AABQQII+AAAUkFV3oMwsnwkA7AoEfQCAdyk1YGP5TboL\nU3cAAKCABH0AACggQR8AAApI0AcAgAJyMy5sI6vqAADdgRF9AAAoICP6AADbwPKbdBdG9AEAoIAE\nfQAAKCBTd+Bd3GgLABSFoA8AUEbm8LOrMHUHAAAKSNAHAIACEvQBAKCABH0AACggN+OyW7GqDgCV\n1pnfRW7YpRyM6AMAQAEZ0adQjNgDALxD0AcA2MVYi59yMHUHAAAKyIg+3YqpOQBgxJ/OEfTZpQjy\nAADlIejTZaY+c3vyTKWrAIDiM+JPsg1z9OfNm5fhw4dn0KBBGT16dBYvXrzV/Z9//vl87nOfy7HH\nHpuhQ4dmxowZaWtr2+GCAQCA0jo1oj9//vxMnTo1kyZNyjHHHJPZs2dn/Pjxeeihh9K/f/9N9n/t\ntdfy+c9/Ph/4wAdy66235tlnn82tt96a6urqjB8/vuwXwa7BtBsA6B469Tu7xF/h/VVg11cy6Le1\ntaW+vj6jRo3K5MmTkySnnnpqRowYkVmzZuXqq6/e5Ji5c+dm3bp1mT59enr27JnTTz89a9euzYwZ\nM3LhhRemR48e5b8SdjpBHgDYyPSgXV/JoL98+fKsXLkyw4YNa2/r0aNHhg4dmscff3yzxzzxxBM5\n5ZRT0rNnz/a2j33sY5k+fXr+7d/+LUOGDClD6bybEA4A7Ep2NJv4oLDjSgb9ZcuWJUkOPfTQDu39\n+/fPihUrsn79+lRXV29yzEknnbTJ/hu3bU/QX7JkyTYfUw4tLS2Z9v/9/Vb/fPU3gy4r+TpTn7m9\njFUBABTbrjCI2ZmMtzUtLS1JKpdjSwb9pqamJElNTU2H9pqammzYsCEtLS3p1avXJsdsbv93v962\n+uMf/7hdx5XDlUd+YavbO1NbqdcAAGDXUq78Wakc26k5+klSVVW12e1bat+SPfbY9ofxHnfccdt8\nDAAA7M5Kpu7a2tokSXNzc4f25ubmVFdXbzJynyS9evXa7P4btwEAADtXyaC/cW5+Y2Njh/bGxsYM\nGDBgs8cMGDAgL7300ib7J8nhhx++PXUCAADboGTQHzBgQPr165cFCxa0t7W2tubRRx/NKaecstlj\nTj755DzxxBMd5iMtWLAg++67bz74wQ+WoWwAAGBrqr/xjW98Y2s7VFVVpUePHmloaEhra2vWrl2b\n66+/Pi+++GK+/e1vp3fv3lmxYkV+//vf56CDDkryzqj97Nmz8+STT6ZPnz55+OGHM3369Fx66aU5\n4YQTuuK6AABgt1bVtvFu2xLuueee3HfffXn99ddTV1eXK6+8Mscee2ySZMqUKZk/f36WLl3avv+/\n/du/5Vvf+laeffbZ9O3bN5/+9KfzxS9+cedcBQAA0EGngz4AANB9bPtalwAAwC5P0AcAgAIS9AEA\noIAEfQAAKCBBfzvdcccdOeqooypdRrf02GOP5bzzzsvgwYMzfPjwzJ49O+4J33aLFi3KBRdckOOP\nPz6nnXZarrjiiqxevbrSZXVbTU1NOeOMM/Lwww9XupRuY968eRk+fHgGDRqU0aNHZ/HixZUuqVv7\nxS9+0b6aHdtm/fr1uffee3PWWWdl8ODB+fM///PMmTPH75btsHbt2txyyy0544wzMnjw4Fx44YV5\n9tlnK11Wt7V27dqcddZZmTJlSkXOL+hvh+effz7f+c53Kl1Gt7R48eJMnDgxAwcOTENDQ84///xM\nmzYts2bNqnRp3coLL7yQsWPHpqamJjfddFOuvPLKLFq0KOPHj09ra2uly+t2mpqacskll+QPf/hD\npUvpNubPn5+pU6dm5MiRqa+vT21tbcaPH7/JU9TpnEWLFuWrX/1qpcvothoaGnLzzTdn5MiRmT59\nes4666xcd911ueuuuypdWrdz/fXXZ/bs2ZkwYULuvPPO9OzZMxdeeGFWrlxZ6dK6pTvuuCMvvvhi\nxc6/Z8XO3E2tX78+V111Vfbbb7+88sorlS6n25k5c2aOPPLIXHfddamqqsqpp56aF198MXPnzs3Y\nsWMrXV63MWfOnBxwwAGpr69Pjx49kiSHHnpozj///DzxxBM5/fTTK1xh97Fw4cJMnTo1r732WqVL\n6Tba2tpSX1+fUaNGZfLkyUmSU089NSNGjMisWbNy9dVXV7jC7mPt2rWZNWtWbrvttuyzzz4+qG+H\njaP548ePz8SJE5Mkp5xyStasWZN77rknEyZMqHCF3cebb76Z73//+/nyl7+cz3zmM0mS4447Lied\ndFIeeuihXHLJJRWusHv5j//4j8yePTt9+vSpWA1G9LfRzJkz09zcnM9+9rOVLqVbmjJlSm6++eZU\nVVW1t/Xo0SNr166tYFXdz5FHHplx48a1h/zknSdSJ8lLL71UqbK6pUmTJmXgwIFG/rbB8uXLs3Ll\nygwbNqy9rUePHhk6dGgef/zxClbW/Tz22GOZMWNGrrjiCr9XtlNTU1POPffcDB8+vEP7YYcdljVr\n1uSPf/xjhSrrfnr27Jl58+blk5/8ZHvbnnvumaqqKr+nt9G6dety1VVXZfz48Xnf+95XsTqM6G+D\n5cuXp76+PnfddVf+/d//vdLldEv9+vVr//cbb7yRRx55JA8++GD7KAydM2bMmE3aHnnkkST/G/jp\nnLlz52bgwIE+IG2DZcuWJXnnr0jv1r9//6xYsSLr169PdXV1BSrrfo455pj84he/yHvf+97U19dX\nupxuqXfv3rnmmms2af/lL3+Zgw46KPvss08Fquqe9txzz3zoQx9KkmzYsCErV65MfX19qqqqMnLk\nyApX1738/d//fVpbW/PFL34xP//5zytWh6CfpLW1NStWrNji9r59++a9731vrr766pxzzjk5/vjj\nBf3N6Ew/9u7dO0k6jAYeffTR+fSnP90lNXYH29KPG61atSo33HBDjj766Jx88sk7u8RuobP9OHDg\nwC6sqhiampqSJDU1NR3aa2pqsmHDhrS0tKRXr16VKK3bqeRIX5F9//vfzxNPPGEa2Q5oaGho//B5\n2WWXGUTaBi+88EK+853vZObMmdlrr70qWougn+SVV17Jn//5n29x+1//9V/nPe95T5YvX57p06d3\nYWXdS2f6ceM8/F69emXWrFlZvXp1brvttowePToPPvhgevbs2UXV7rq2pR+Td0L+2LFjs2HDhtxy\nyy0dpkXtzra1H+m8jSuZbOl7zfcglfTDH/4wU6dOzZlnnmk61A742Mc+lhNPPDG//e1v09DQkNbW\n1lx++eWVLmuXt2HDhnzta1/Lpz71qV1iFS1BP8n73//+LF26dIvbV61albPPPjvXX3999t5776xb\nt679F926deuyxx57ZI893O5Qqh/frXfv3u0jzx/4wAcycuTI/PSnP8255567M0vsFralH59//vlM\nmDAh69atyz333JNDDjlkJ1fXfWxLP7JtamtrkyTNzc3p27dve3tzc3Oqq6s3GemHrnLvvffm29/+\ndoYNG5a/+7u/86FzB3zwgx9Mkpx44olpbm7O3XffnUmTJnW4N4xNzZ49O6tWrcqMGTOybt269va2\ntrasW7cue+7ZtdFb0O+EJ598Ms3Nzbnssss22fbhD384kydPzqWXXlqByrqfBQsW5MADD8ygQYPa\n2wYOHJgePXrk1VdfrWBl3c/TTz+dL3zhC6mtrc2sWbMyYMCASpfEbmLj3PzGxsYO8/QbGxt9H1Ix\nN998c7773e/m3HPPzbe+9a0uD1RF8F//9V957LHHcuaZZ3aYfldXV5e1a9fmv//7v3PAAQdUsMJd\n34IFC/Lyyy/nhBNO6ND+3HPP5cEHH8wvfvGLvP/97++yevwUdMIZZ5yRBx54oEPbj3/849x77715\n4IEHcuCBB1aosu5nxowZec973pPZs2e3t/3mN79Ja2urudLboLGxMRMmTEjfvn0zc+ZM83zpUgMG\nDEi/fv2yYMGCnHbaaUneuSfi0UcfzdChQytbHLulWbNm5bvf/W4uvPDCXHXVVUbyt9Mbb7yRq666\nKkly3nnntbf/+te/zv7775/999+/UqV1G3/zN3+T5ubmDm1f+cpXcthhh2XSpEldnhkF/U7o06fP\nJmugPvXUU0neWTGBzrv44oszceLEXHPNNTnrrLPy+9//PrfffntOPPFEa79vg+uuuy5NTU255ppr\nsmrVqqxatap928EHH+zDJztVVVVVJkyYkGuvvTa9e/fOkCFDMmfOnLz++uvue6DLvfrqq/m7v/u7\nDBw4MGeffXaefvrpDtuPPvpoo/uddMQRR+TMM8/Mt7/97bS2tqZ///752c9+loceeijXXXedacqd\nsLmblvfee+/su+++FcmMvvPpUsOGDUtDQ0MaGhry0EMPpba2Nuecc04uv/xyIzCd1Nramsceeyzr\n16/Pl7/85U22X3HFFRk/fnwFKmN3MmbMmLz99tu57777MnPmzNTV1eXuu+9O//79K10au5l/+Zd/\nydq1a/P8889n9OjRm2x/8skns99++1Wgsu7p29/+du64447MmDEjr776ao488sjcdtttGTFiRKVL\nYztUtW28qxQAACgMf4MBAIACEvQBAKCABH0AACggQR8AAApI0AcAgAIS9AEAoIAEfQAAKCBBHwAA\nCuj/B45aFBBSvX1vAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2289474ca20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "norm=np.random.normal(size=100000)\n",
    "unif=2*(np.random.rand(100000)-.5)\n",
    "\n",
    "plt.hist(unif, bins=np.arange(-4,4,.1), normed=True)\n",
    "plt.hist(norm, bins=np.arange(-4,4,.1), normed=True)\n",
    "plt.legend([\"Uniform\", \"Normal\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caption: Yep, the standard normal distirbution assigns lower probabilites to [-1,1] than a uniform over the same points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "The KL divergence takes in a data-generating process P (so, a distribution over possible outcomes), and an alternative distribution Q. It tells us how much needless surprise we'd experience if we believed the data were generated by Q instead of by P.\n",
    "\n",
    "While KL can be used as an abastract measure of the level of difference between two distributions, it's much better to think in terms of a true/assumed data generating distribution and an approximate alternative distribution.\n",
    "\n",
    "#### Applications\n",
    "In frequentist statistics, the KL-divergence is related to the maximum likelihood (both pick a model that minimizes surprise at the data). In Bayesian statistics the KL divergence can be used as a measure of the information gain in moving from a prior to posterior (with a common goal in Bayesian experimental design to maximise the expected KL divergence between the prior and the posterior). The divergence is also used to understand mutual information in clustering, and in variational bayesian inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Models\n",
    "\n",
    "We can use KL-Divergences of two different models to do model comparison. We want to pick the model whose KL divergence from the true data-generating distribution $P$ is as small as possible. If we work through the math,\n",
    "\n",
    "$$\\kld(P, Q) -\\kld(P, R) = E_P[log(p)-log(q)] - E_P[log(p) - log(r)] \\\\= E_p[log(r) - log(q)] \\\\= E_p[log(\\frac{r}{q})]$$\n",
    "\n",
    "In the sample approximation we have:\n",
    "\n",
    "$$\\kld(P, Q) -\\kld(P, R) = \\frac{1}{N} \\sum_i log(\\frac{r_i}{q_i}) = \\frac{1}{N} log(\\frac{\\prod_i r_i}{\\prod_i q_i}) =  \\frac{1}{N}log(\\frac{\\cal{L}_r}{\\cal{L}_q})$$\n",
    "\n",
    "So to pick the model with smaller KL divergence, we can pick the model with the larger likelihood. This should be comforting, as picking the lower-KL model ties to comparing likelihood ratios.\n",
    "\n",
    "Those familar with statistics know that examining the likelihood ratios of two models is an extremely powerful means of testing them. (And likelihood ratio testing happens to be equivalent to saying \"I'll reject a parameter setting if it generates M bits more surprise than the MLE (minimum surprise) estimate on the observed data; otherwise that parameter setting is plausible\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus material\n",
    "\n",
    "### Another view: Cross Entropy\n",
    "\n",
    "Cross-Entropy is defined as\n",
    "\n",
    "$$H(P, Q) = - E_p[log(q)] = -\\int log(q(x))\\ p(x)dx$$\n",
    "\n",
    "It measures the average surprise accumulated by Q across all points generated by P.\n",
    "\n",
    "Since KL divergence is EXCESSS surprise, one can unsurprisingly write\n",
    "\n",
    "$$\\kld(p, q) = H(p,q) - H(p) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tie to Maximum Likelihood\n",
    "We've already said that maximum likelihood picks distribution (among those available by tweaking the parameters) that is least surprised by the data. This is asympotically the same as picking the available distribution with lowest KL divergence from the true data generating process. (Intuitively, picking the distribution that is least surprised by the data [ML] is the same as picking the distribution that experiences least excess surprise [KL])\n",
    "\n",
    "**Argument**:\n",
    "$$\\kld(P, Q) = E_P[log(p/q)] = E_P[-log(q)] - E_P[-log(p)] $$\n",
    "\n",
    "The rightmost term is the entopy of the data generating process- nothing can be done about that it's just a fixed value set by how crazy the true data distribution is. Minimizing KL is the thus about minimizing the first term - minimizing the average surprise Q experiences across points generated by P.\n",
    "\n",
    "However, we don't often have the true data distribution P (and if we did, we wouldn't need to approximate it with a model), we have samples from it. We hope, via LLN, and numerous enough samples that\n",
    "\n",
    "$$\\frac{1}{N}\\sum_i -log(q_i) \\rightarrow E_P[S(q)]$$\n",
    "\n",
    "So, minimizing KL is about minimizing the term on the left. And the term on the left is the negative log likelihood with a 1/N in front, so [in the limit of large sample sizes] minimizing KL is the same as maximizing likelihood, and vice versa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof: KL is always positive, and 0 only if Q is the same as P\n",
    "\n",
    "We can use Jensen's inequality for expectations on a convex function $f(x)$, \n",
    "\n",
    "$$ E[f(X)] \\ge f(E[X]) $$\n",
    "\n",
    "to show that $\\kld(p,q) \\ge 0$ with equality iff (if and only if) $q=p$.\n",
    "\n",
    "$$\\kld(p,q) = E_p[-log(q/p)] \\ge -\\log \\left( E_p[q/p] \\right)$$\n",
    "$$= -\\log(\\int \\frac{q(x)}{p(x)}p(x)\\,dx) = -\\log(\\int q(x) dx) = 0$$\n",
    "\n",
    "where we have used the fact that $-log(x)$ is a convex function, and that $q(x)$ normalizes to a distribution. Infact, since $-\\log(x)$ is strictly convex, the equality only happens if $q(x) = p(x)$ for ALL x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
