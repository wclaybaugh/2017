{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of learning\n",
    "\n",
    "##### Keywords: generative model, supervised learning, semi-supervised learning, unsupervised learning, mixture model, gaussian mixture model, latent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\newcommand{\\isum}{\\sum_{i}}$$\n",
    "$$\\newcommand{\\zsum}{\\sum_{k=1}^{K}}$$\n",
    "$$\\newcommand{\\zsumi}{\\sum_{\\{z_i\\}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning\n",
    "\n",
    "In **supervised learning**, we have some training data of $N$ data points, each with $m$ features. The big idea is that on the training set, we have **a label associated with each data point**. Here the label is $z$.\n",
    "\n",
    "For a feature vector x, we use Bayes' rule to express the posterior of the class-conditional as:\n",
    "\n",
    "$$p(Z = c \\vert X=x, \\theta) = \\frac{p(Z = c  \\vert  \\theta)p(X=x  \\vert  Z = c, \\theta)}{ \\sum_{c′} p(Z = c′  \\vert  \\theta) p(X=x  \\vert  Z = c′, \\theta)}$$\n",
    "\n",
    "This is a **generative classifier**, since it specifies how to generate the data using the class-conditional density $p(X=x \\vert Z = c, \\theta)$ and the class prior $p(Z = c\\vert \\theta)$. [Given the parameters $\\theta$ draw a particular class c; given that class, draw x according to $p(X=x \\vert Z = c, \\theta)$\n",
    "\n",
    "Notice that even though we are talking about priors and posteriors here, I am carrying along $\\theta$. This is because the models here are just probabilistic models, and we haven't chosen a frequentist or bayesian modelling paradigm yet. We are just specifying the model, and the priors and posteriors we are talking about here are simply those from bayes theorem.\n",
    "\n",
    "An alternative approach is to directly fit the class posterior, $p(z = c \\vert x, \\theta)$; this is known as a **discriminative classifier**. For example, a Gaussian Mixture model or Naive bayes is a generative classifier whose discriminative counterpart is the logistic regression.\n",
    "\n",
    "**The supervised learning case is the one in which where hidden variables $z$ are known on the training set**. So the supervized case is one in which the model is not a hidden variables model at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning: Mixture of Gaussians \n",
    "\n",
    "In unsupervised learning, we do not know the class labels. We wish to generate these labels automatically from the data. An example of an unsupervised model is clustering.  In the context of mixture models we then do not know what the components of the mixture model are, i.e. what the parameters of the components and their admixture ($\\lambda$s) are. Indeed, we might not even know how many components we have!!\n",
    "\n",
    "In this case, to carry out the clustering, we first fit the mixture model, and then compute $p(z_i = k  \\vert  x_i, \\theta)$, which represents the posterior probability that point i belongs to cluster k. This is known as the responsibility of cluster k for point i, and can be computed as before using Bayes rule as follows:\n",
    "\n",
    "$$p(z_k = c \\vert x_i, \\theta) = \\frac{p(z_k = c  \\vert  \\theta)p(x_i  \\vert  z_k = c, \\theta)}\n",
    "{ \\sum_{_c′} p(z_k = c′  \\vert  \\theta) p(x_i  \\vert  z_k = c′, \\theta)}$$\n",
    "\n",
    "This is called soft clustering. K-means is a hard-clustering analog where you associate a data point with a cluster rather than simply computing probabilities for the association.\n",
    "\n",
    "The process is identical to the computations performed before in the supervised learning, except at training time: here we never observe $z_k$ for any samples, whereas before with the generative GDA classifier, we did observe $z_k$ on the training set.\n",
    "\n",
    "How many clusters? The best number will generalize best to future data, something we can use cross-validation or other techniques to find.\n",
    "\n",
    "Put differently, **unsupervised learning is a density estimation problem for $p(x)$**.\n",
    "\n",
    "$$p(x) = \\sum_c p(x \\vert z) p(z).$$\n",
    "\n",
    "In other words we discover the marginal p(x) through the generative model $p(x \\vert z)$. This is also very useful in discovering **outliers** in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concretely formulating the problem\n",
    "\n",
    "So let us turn our attention to the case where we do not know the labels $z$. \n",
    "\n",
    "Suppose we are given a data set $\\{x_1,\\ldots, x_m\\}$ but not given the labels $z$. The model consists of \n",
    "$k$ Gaussians. In other words our model assumes that each $x_i$ was generated by randomly choosing \n",
    "$z_i$ from $\\{1, \\ldots, k\\}$, and then $x_i$ is drawn from the corresponding gaussian.\n",
    "\n",
    "We wish to compute either the maximum likelihood estimate for this model, $p(\\{x_{i}\\} \\vert \\theta)$.\n",
    "The goal is to model the joint distribution $p(\\{x_i\\}, \\{z_i\\})=p(\\{x_i\\} \\vert \\{z_i\\}) \\, p(\\{z_i\\})$ where $z_i \\sim \\rm{Categorical}(\\lambda)$, and $\\lambda = \\{\\lambda_j\\}$.\n",
    "\n",
    "As in our definition of mixture models $\\lambda_j\\ge0$  and \n",
    "\n",
    "$$ \\sum_j^k \\lambda_i = 1 $$\n",
    "\n",
    "The parameters $\\lambda_j$  produce $p(z_i=j)$ and then $x_i \\vert z_i=j \\sim {\\cal N}(\\mu_j, \\Sigma_j)$.\n",
    "\n",
    "The parameters of our problem are $\\lambda$, $\\mu$ and $\\Sigma$. But because the labels $z$ are hidden to us, we no longer have the full-data likelihood. So we estimate our parameters by minimizing the $x$-data log-likelihood:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "l(x \\vert  \\lambda, \\mu, \\Sigma) &=& \\sum_{i=1}^{m} \\log p(x_i \\vert  \\lambda,  \\mu ,\\Sigma)   \\nonumber \\\\ \n",
    "     &=& \\sum_{i=1}^{m} \\log \\sum_z p(x_i \\vert  z_i,  \\mu , \\Sigma) \\, p(z_i \\vert  \\lambda)  \n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "However, if we set to zero the derivatives of this formula with respect to\n",
    "the parameters and try to solve, we'll find that it is not possible to find the\n",
    "maximum likelihood estimates Futhermore, we have to enforce constraints such as mixing weights summing to 1, covariance matrices being positive definite, etc. \n",
    "\n",
    "For all of these reasons, its simpler, but not always faster to use an iterative algorithm called the EM algorithm to get the local maximum likelihood or MAP estimate. We shall learn this algorithm soon. But we can also set this problem up as a bayesian problem with reasonable priors on the parameters and try to use MCMC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Semi-supervised\n",
    "\n",
    "In unsupervized learning we are given samples from some unknown data distribution with density $p(x)$. Our goal is to estimate this density or a known functional  (like the risk) thereof. Supervized learning can be treated as estimating $p(x,c)$ or a known functional of it. But there is usually no need to estimate the input distribution so estimating the complete density is wasteful, and we usually focus on estimating $p(c \\vert x)$ discriminatively or generatively(via $p(x \\vert c) p(c)$. Here $c$ or $y$ or $z$ are the classes we are trying to estimate. In the unsupervized case we often estimate $\\sum_z p(x \\vert z) p(z) = p(x)$ with latent (hidden) $z$, which you may or may not wish to identify with classes.\n",
    "\n",
    "Semi-supervised learning is the situation in which we have some labels, but typically very few labels: not enough to form a good training set.\n",
    "\n",
    "In semi-supervized learning we combine the two worlds. We write a joint likelihood for the supervised and unsupervised parts:\n",
    "\n",
    "$$ l(\\{x_i\\},\\{x_j\\},\\{z_i\\} \\vert \\theta, \\lambda) = \\sum_i log \\, p(x_i, z_i \\vert \\lambda, \\theta) +  \\sum_j log \\, p(x_j \\vert \\lambda, \\theta) = \\sum_i log \\, p(z_i \\vert \\lambda) p(x_i \\vert z_,\\theta) + \\sum_j log \\, \\sum_z p(z_j \\vert \\lambda) p(x_j \\vert z_j,\\theta) $$\n",
    "\n",
    "Here $i$ ranges over the data points where we have labels, and $j$ over the data points where we dont.\n",
    "\n",
    "In a traditional classification-based machine learning scenario we might still split the data into a training and validation set. But the basic idea in semi-supervised learning is that there is structure in $p(x)$ which might help us divine the conditionals, so what we would want to do, is include in the training set unlabelled points. The game then is that if there is geometric structure in $p(x)$, some kind of cluster based foliation, then we can explot this."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
