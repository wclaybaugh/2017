{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TL;DR\n",
    "You have **a** dataset; one of many possible datasets. The precise value in row three, column seven of your dataset was partly due to chance (e.g. measurement error). As a result, any mean or correlation or anything you calculate from that dataset has a chance component: with different luck or different measurement errors you'd get a different value. \n",
    "\n",
    "And that should worry you, becuase the question is not \"what is the mean of the data\" but \"What is the mean of the data, what are the error bars on that, and how sure are we that our value is close to anyone else's value, let alone the actual quantity we're trying to measure\".\n",
    "\n",
    "\n",
    "## The Myth of \"The\" Data\n",
    "Suppose you run an experiment. For instance, you invite every 1,000th visitor to your website to fill out a survey, or you measure the growth rate of rats exposed and not exposed to some chemical, or you put together an aparatus to measure the speed of light. You get a dataset, whether it's one column or many, and store it in a file cabinet.\n",
    "\n",
    "Now, suppose you run the same experiment over again. Do you get an identical dataset? Anyone who has ever collected data knows that the answer is \"no\". Try as you might, to a greater or lesser degree you will get different numbers. The entry of row three column seven will not match to infinity decimal places across both datasets.\n",
    "\n",
    "This should be a scary thought for any data analyst: whenever you crunch a number from a dataset, a version of you in a parallel universe got a different dataset and is crunching a different number. How much faith can you really have in the particular value you report?\n",
    "\n",
    "So, careful scientists and statisticians aren't just interested in the particular mean or correlation or ____ computed from thier data, but in the _distribution_ of values that might arise with slightly different luck during data collection. That is, they consider how spread out the possible calculated values might be; is our process such that those parallel universe versions of us are getting similar answers, or is the distribution of answers all over the place?\n",
    "\n",
    "Our goal over this set of notes is to discuss when and how we can be confident that the particular mean or correlation we computed from our dataset is close to the mean or correlation that parallel-universe us computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
