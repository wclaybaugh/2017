{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thermodynamic notion of Entropy\n",
    "\n",
    "Imagine dividing $N$ objects amongst $M$ bins. One can think of this as stone tossing, where we toss N stones and see in which bin they land up. There is a distribution for this, $\\{p_i\\}$, of-course, so lets see what it is.\n",
    "\n",
    "There are $N$ ways to fill the first bin, $N-1$ ways to fill the second, $N-2$ ways to fill the third, and so on...thus $N!$ ways. Since we dont distinguish the arrangement of objects in each bin we must divide bu the factorial of the bin amounts. If we then assume a uniform chance of landing in each bucket, then we just get the nultinomial distribution:\n",
    "\n",
    "$$P(n_1, n_2, ..., n_M) = \\frac{N!}{\\prod_{i} n_i!} \\prod_i (\\frac{1}{M})^{n_i} = \\frac{N!}{\\prod_{i} n_i!} \\left(\\frac{1}{M}\\right)^N$$\n",
    "\n",
    "$$ W =  \\frac{N!}{\\prod_{i} n_i!} $$\n",
    "\n",
    "is called the multiplicity and the entropy is then defined as:\n",
    "\n",
    "$$H = \\frac{1}{N} log(W)$$ which is:\n",
    "\n",
    "$$\\frac{1}{N}log(P(n_i, n_2, ...,n_M))$$\n",
    "\n",
    "with a constant term removed.\n",
    "\n",
    "\n",
    "$$H = \\frac{1}{N} log(N!) - \\frac{1}{N} \\sum_i log(n_i!)$$.\n",
    "\n",
    "Using Stirling's approximation $log(N!) \\sim Nlog(N) -N$ as $N \\to \\infty$ and where the fractions $n_i/N$ are held fixed:\n",
    "\n",
    "$$ H =  \\frac{1}{N}\\left( N log(N) - N - \\sum_i (n_i log(n_i) - n_i)\\right)$$\n",
    "\n",
    "$$ = log(N) -1 -\\frac{1}{N} \\sum_i (Np_i log(Np_i) - Np_i) = log(N) -1 - \\sum_i \\left(p_i(log(N) + log(p_i)) - p_i\\right)$$\n",
    "\n",
    "Thus\n",
    "\n",
    "$$H = -\\sum_i p_i log(p_i)$$\n",
    "\n",
    "If the probabilities of landing in each bucket are not equal, ie not uniform, then we can show:\n",
    "\n",
    "$$\\frac{1}{N}log(P(n_i, n_2, ...,n_M)) = -\\sum_i p_i log(\\frac{p_i}{q_i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This definition has origins in statistical mechanics. Entropy was first introduced in thermodynamics and then later interpreted as a measure of disorder: how many events or states can a system constrained to have a given enrgy have. A physicist calls  a particular arrangement $\\{n_i\\} = (m_1, n_2, n_3,...,n_M)$ a microstate and the overall distribution of $\\{p_i\\}$, here the multinomial , a macrostate, with $W$ called the weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
