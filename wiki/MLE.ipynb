{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation\n",
    "\n",
    "##### Keywords: maximum likelihood,  parametric model, linear regression, logistic regression, inference,  exponential distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "{:.no_toc}\n",
    "* \n",
    "{: toc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation\n",
    "Maximum Likelihood Estimation is, by far, the best-stuided and most widely applied model fitting technique. As usual, the aim is to work out which set of parameter values we ought to use in our model.\n",
    "\n",
    "Maximum Likelihood Estimation says that we should pick the parameter values that make the data most likely to arise. We'll see later on that this is equivalent to picking the parameter values that are least surprised by the data.\n",
    "\n",
    "To apply the MLE procedure:  \n",
    "1) Write down the (log) likelihood function.  \n",
    "2) Either analytically or numerically optimize the likelihood by varying the parameters and keeping the observed data fixed.  \n",
    "3) Use the values of the parameters obtained in step 2 as the offical MLE best-guess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Via table\n",
    "\n",
    "Recall our 'spreadsheet' of all possible datasets and all possible parameter settings:\n",
    "\n",
    "| |prameter setting 1| parameter setting 2|...|\n",
    "|-|-|-|-|\n",
    "|**dataset 1**|$P($dataset1$\\vert$parameter setting1$)$|$P($dataset1$\\vert$parameter setting2$)$|...|Row Sum=?\n",
    "|**dataset 2**|$P($dataset2$\\vert$parameter setting1$)$|$P($dataset2$\\vert$parameter setting2$)$|...|Row Sum=?\n",
    "|**dataset 3**|$P($dataset3$\\vert$parameter setting1$)$|$P($dataset3$\\vert$parameter setting2$)$|...|Row Sum=?\n",
    "|...|...|...|...|...\n",
    "| |Column Sum=1|Column Sum=1|...\n",
    "\n",
    "Each entry in the table is the likelihood of the particular dataset. To find the MLE we look up our dataset in the table (let's say it's on row 2) and then pick the column with the largest value in that row.\n",
    "\n",
    "Of course, the number of possible parameter settings tends to be [highly] infinite, so we tend to resort to calculus to find the best set of parameters, holding the data constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### via Graph\n",
    "\n",
    "The diagram below illustrates the idea behind the MLE.\n",
    "\n",
    "![](images/gaussmle.png)\n",
    "\n",
    "Consider a likelihood funtion using the parameter value 1.8 (blue) and the same likelihood but with a parameter value of 5.8. (green). Let's say we have 3 data points, at $x=1,2,3$.\n",
    "\n",
    "Maximum likelihood says we should favor whichever value of the parameter, if true, makes the data more likely to occur.\n",
    "\n",
    "In our case the blue parameter value is more likely since the product of the height of the 3 vertical blue bars is greater than the product of the 3 green bars.\n",
    "\n",
    "Indeed the question that MLE asks is: how can we move and scale the distribution by changing the parameters, so that the product of the 3 bars is maximised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Via math\n",
    "If we assume that the overal likelihood of the dataset is the product of the likelihood of each row (i.e. the rows are drawn independently), we can write the likelihood as\n",
    "\n",
    "$$\n",
    "P(dataset\\,|\\,parameters)=L(\\theta) = \\prod_{i=1}^n P(x_i \\mid \\theta)\n",
    "$$\n",
    "\n",
    "This gives us a measure of how likely it is to observe values $x_1,...,x_n$ given the parameters $\\theta$. Our goal is to maximize the expression above by picking $\\theta$. Remember that we're working within a single row: $x_i$ [the observed data] are considered fixed.\n",
    "\n",
    "Often it is easier and numerically more stable to maximise the log likelyhood:\n",
    "\n",
    "$$\n",
    "\\ell(\\lambda) = \\sum_{i=1}^n ln(P(x_i \\mid \\theta))\n",
    "$$\n",
    "\n",
    "Because log is a monotonic transformation if the highest likelihood occurs at $\\theta=.07$ before we take the log it will still occur at $\\theta=0.7$ after we take the log, and the sum is much easier to work with than the product.\n",
    "\n",
    "From here, we take the derivative of the log-likelihood and hunt for places where the derivative [or gradient] is zero of undefined as candidate maxima, or we resort to numerical techniques to find a local maximum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: MLE for an Exponential Distribution\n",
    "The exponential distribution occurs naturally when describing the lengths of the inter-arrival times in a homogeneous Poisson process.\n",
    "\n",
    "It takes the form:\n",
    "$$\n",
    "f(x;\\lambda) = \\begin{cases}\n",
    "\\lambda e^{-\\lambda x} & x \\ge 0, \\\\\n",
    "0 & x < 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "#### Finding the MLE\n",
    "In this example the observed data are $n$ scalar values $x_i$, and the model is that each point is an iid draw from an exponential distribution with unknown paramter lambda.\n",
    "\n",
    "We have,\n",
    "$$\n",
    "log(P(data\\,|\\,parameters)=\\ell(\\lambda)) = \\sum_{i=1}^n ln(\\lambda e^{-\\lambda x_i}) = \\sum_{i=1}^n \\left( ln(\\lambda) - \\lambda x_i \\right).\n",
    "$$\n",
    "\n",
    "Maximizing this:\n",
    "\n",
    "$$\n",
    "\\frac{d \\ell}{d\\lambda} = \\frac{n}{\\lambda} - \\sum_{i=1}^n x_i = 0\n",
    "$$\n",
    "\n",
    "and thus:\n",
    "\n",
    "$$\n",
    "\\est{\\lambda_{MLE}} = \\frac{n}{\\sum_{i=1}^n x_i}\n",
    "$$\n",
    "\n",
    "From here we just need to sum the $x_i$ and plug in. We thus have a way of finding the MLE best-guess for $\\lambda$ for any dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nice Properties\n",
    "\n",
    "#### Asymptotic Normality\n",
    "In the above examples, we considered the observed data to be fixed. But we know that each of the $x_i$ in the dataset is really a random variable, i.e. we could have easily ended up with a different dataset / on another row of the table.\n",
    "\n",
    "Thus the particular value of the MLE parameters we calculate depends on our luck during data collection. What does the spread of possible MLE results look like? It turns out that *with enough data, the possible MLE results look more and more like a normal distribution centered on the true parameter value (if there is one) and with variance that decreases as 1/n*. So with a good amount of data, odds are that our observed MLE isn't too far from the target MLE, and we could even work out probabilities of being off by a given amount. If the there is no true parameter value, the MLE is still normally distributed, and centered on the parameters that are closest to the true model (in a particular information-theory measurement)\n",
    "\n",
    "### Effeciency, etc\n",
    "The MLE has a host of other nice features like \"doesn't leave any information in the data lying on the table\". If you want to know more about when, how, and why MLE works well (or doesn't) take a class on statistical inference."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
