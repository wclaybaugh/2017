{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "So far, we discussed functions from $\\Omega$ to $R$. I.E. we talked about mappings from whatever the heck happened to what [single] number got written down, for example the mapping from every possible set of events in measuring someone's weight (including time of day, whether the scale is broken, etc etc) into the number we write down on our sheet. We then said that the person's weight was a Random Variable, and described it via some [one dimensional] pdf or cdf. We put in a weight (or range of weights in the continuous case), we find out the probabilty of observing that weight. \n",
    "\n",
    "But what if we wrote down the person's height and their weight? Or their height, weight, and age? Now we have TWO or THREE random variables, and there could be all kinds of structure linking outcomes together. We need a richer theory to describe it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint Distributions\n",
    "Joint distributions are the fullest, richest thing we can have for a set of variables. We provide an outcome for each variable (e.g. 163-165 cm, 100-110kg, 19-20 years old) and get back a probability. [Though, as usual, of we have a pdf we need to do an integral to get a probability]\n",
    "\n",
    "In symbols, joint pdfs are $p(X = x_i, Y= y_j)$, the probability that big x takes value $x_i$ and at the same time big y takes the value $y_j$.\n",
    "\n",
    "Note that the sum/integral across all of a joint distribution is 1. The joint distribution covers all of $\\omega$\n",
    "\n",
    "### Continuous Joint Distributions\n",
    "As an example, the joint distribution for two variables coming from the multivariate gaussian is:\n",
    " $$P(x_1,x_2)=\\frac{1}{2\\pi\\sigma_1\\sigma_2 \\sqrt{1-\\rho^2}}e^{-z/(2(1-\\rho^2))}$$  \n",
    "where $z=\\frac{(x_1-\\mu_1)^2}{\\sigma_1^2}-\\frac{2\\rho(x_1-\\mu_1)(x_2-\\mu_2)}{\\sigma_1\\sigma_2}+\\frac{(x_2-\\mu_2)^2}{\\sigma_2^2}\\\\$.  \n",
    "$\\mu_1,\\sigma1,\\mu2,\\sigma2,$ and $\\rho$ are all paramaters of the distribution.\n",
    "\n",
    "The point is, though, that if we provide the $\\mu$s, $\\sigma$s, $\\rho$ and $x_1$ and $x_2$ we can get back a probability (via integration).\n",
    "\n",
    "### Discrete Joint Distributions\n",
    "Discrete joint distributions are simply functions that map values of each variable $x_1,x_2,x_3...$ to a probability. For small dimension they can be written out as a table, e.g. for spookily correlated spinner (lands either Red, Blue, or Green) and 6-sided die we may have:\n",
    "\n",
    "|result|Red|Green|Blue|\n",
    "|-|-|-|-|-|-|-|-|\n",
    "|**1**|.1|.0|.0|\n",
    "|**2**|.0|.1|.2|\n",
    "|**3**|.1|.0|.0|\n",
    "|**4**|.0|.1|.0|\n",
    "|**5**|.1|.0|.2|\n",
    "|**6**|.0|.1|.0|\n",
    "Which says that (because of magnets) [Red,Odd] pairings are common, [Green,Even] pairings are common, and Blue pairs with 2s or 5s.\n",
    "\n",
    "The point, though, is that we provide the value of the die and a value of the spinner and we get back a probability.\n",
    "\n",
    "\n",
    "### Higher dimensions\n",
    "The above generalize well to higher dimensions. In high dimensions the table may become 3d or worse, but we still plug in values of each outcome variable and get back a probability. For the continuous PDF there may be more parameters and a more tedious integral (e.g. 4d instead of 2d), but the process is the same.\n",
    "\n",
    "### Joint CDFs\n",
    "The above examples were joint PDFs. Joint CDFs also exist and are arguably more fundamental, but not important for this course. The important idea is that a joint CDF evaluated at (3,7) gives the total probability of seeing $x_1<3$ and $x_2<7$ in whatever combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginal Distributions\n",
    "What if we're not especially interested in the intricate interelationships between our several variables? What if we only care about the die in the game, or only care about people's heights, not their weights or ages? In this case we can build the correct 1-dimensional distribution from the joint distribution.\n",
    "\n",
    "### Discrete Marginal Distributions\n",
    "Starting with the dice/spinner above, let's find the probaility distribution of the die, ignoring the spinner. \n",
    "\n",
    "What's the chance that this die comes up 1? Well, there's only one way for that to happen: the spinner is green and the die is 1, and that has a 10% chance.\n",
    "\n",
    "What's the chance that the die comes up 2? Well, there's two ways for that to happen, both listed in the die=2 row: we have to either hit the .1 of (2,green) or the .2 of (2,blue), and these are disjoint events [The spinner can't be both green and blue on the same outcome) so we can just add and get .3\n",
    "\n",
    "In fact, **we can get the marginal, or \"ignoring everything else\" probability of the die equalling any number by simply adding up all entries in that row**. This is becuase the joint distribution breaks out each possible (disjoint) outcome and tells us the probability of that outcome. To find the chance of die=5 or spinner=green we can just run down that row or column adding values.\n",
    "\n",
    "The name \"marginal\" in fact comes from the margin of the table being a good place to write these row/column sums.\n",
    "\n",
    "In symbols,\n",
    "$p(X=x_i) = \\sum_j p(X=x_i, Y=y_j)$, which says that, for each value of X, we should just sum up the PDF over the remaining variable.\n",
    "\n",
    "To end the example, the marginal distribution for the die is:\n",
    "\n",
    "|1|2|3|4|5|6|\n",
    "|-|-|-|-|-|-|\n",
    "|.1|.3|.1|.1|.3|.1|\n",
    "\n",
    "And the spinner's marginal is:\n",
    "\n",
    "|Red|Green|Blue|\n",
    "|-|-|-|\n",
    "|.3|.3|.4\n",
    "\n",
    "\n",
    "### Continuous Marginal Distributions\n",
    "The sum becomes an integral but the idea is unchanged. Wikipedia has a good example calculation if you need it.\n",
    "\n",
    "### More Variables\n",
    "It's vital to understand how to work with joints with multiple variables. Luckily $p(X=x,Y=y,Z=z)$ is enough to see the patterns.\n",
    "\n",
    "$$p(X=x_i) = \\sum_{j,k} p(X=x_i, Y=y_j, Z=z_k)$$\n",
    "$$p(Y=y_j) = \\sum_{i,k} p(X=x_i, Y=y_j, Z=z_k)$$\n",
    "$$p(Z=z_k) = \\sum_{i,j} p(X=x_i, Y=y_j, Z=z_k)$$\n",
    "\n",
    "All of the above say \"to get the marginal, hold the variable you care about fixed, and sum/integrte over all the other variables\". So P(X=7) is found by visiting all places where X=7 and totaling those probabilities.\n",
    "\n",
    "$$p(X=x_i,Z=z_k) = \\sum_j p(X=x_i, Y=y_j, Z=z_k)$$\n",
    "$$p(X=x_i,Y=y_j) = \\sum_k p(X=x_i, Y=y_j, Z=z_k)$$\n",
    "$$p(Y=y_j,Z=z_k) = \\sum_i p(X=x_i, Y=y_j, Z=z_k)$$\n",
    "\n",
    "It's also possible to find \"marginals\" for TWO or more variables of interest. (Though these will be a joint distribution on just those two variables). All of the above say \"to get the marginal, hold the variable**s** you care about fixed, and sum/integrte over all the other variables\". So P(X=7,Y=2) is found by visiting all places where X=7 and Y=2 and totaling those probabilities.\n",
    "\n",
    "This leads to a shortcut: If we only want the probability of a specific event, e.g. $P(X=7)$ instead of $P(X=x)$ for any value of x, we can find just the sum of the X=7 row, or do our sum/integral with X=7 plugged in. To get the distribution of X=x, though, we need to leave X as a variable and plug in/choose a row after the fact.\n",
    "\n",
    "### Summary\n",
    "Marginal distributions are fully-fledged distributions in thier own right. They tell us how one or more variables behave, totally ignoring the others. So, given a joint distribution the \"ignore a variable\" operation is to sum/integrate that variable out.\n",
    "\n",
    "Marginal distributions throw away information. It is not possible to reconstruct a joint distribution from marginals alone; by summing out e.g. Z we lose any information about how the remaining variables depend on Z. For any given marginals there are lots of joint distributions that could have produced those marginals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Distributions\n",
    "Conditional distributions are marginal distribution's more sucessful sibling. They throw away much less data and come up extrmely often in practice.\n",
    "\n",
    "The idea of a conditional distribution is that we don't want to get rid of a variable entirely, but we're not especailly interested in how that variable got its value. For instance, we may want to know what the chance is of a patient developing diabetes GIVEN their age and weight and so on. We don't really care about the probability of them getting to where they are; we just want to know the probability of diabetes arising now that they're here.\n",
    "\n",
    "### Discrete Conditional Distributions\n",
    "The conditional distribution is much richer than the marginal distribution becuase it doesn't eleminate a dimension. So we're going to be looking at a 2d table. But the joint IS a 2d table. So what changes?\n",
    "\n",
    "Answer: we normalize by the row (or column) sums.\n",
    "\n",
    "Original:\n",
    "$$P(Spinner,Die)$$\n",
    "\n",
    "|result|Red|Green|Blue|\n",
    "|-|-|-|-|-|-|-|-|\n",
    "|**1**|.1|.0|.0|\n",
    "|**2**|.0|.1|.2|\n",
    "|**3**|.1|.0|.0|\n",
    "|**4**|.0|.1|.0|\n",
    "|**5**|.1|.0|.2|\n",
    "|**6**|.0|.1|.0|\n",
    "\n",
    "\n",
    "\n",
    "Spinner result GIVEN die result:\n",
    "$$P(Spinner|Die)$$\n",
    "\n",
    "|result|Red|Green|Blue|\n",
    "|-|-|-|-|-|-|-|-|\n",
    "|**1**|1|.0|.0|\n",
    "|**2**|.0|.33|.66|\n",
    "|**3**|1|.0|.0|\n",
    "|**4**|.0|1|.0|\n",
    "|**5**|.33|.0|.66|\n",
    "|**6**|.0|1|.0|\n",
    "\n",
    "Let's look at how this table works: it still takes in both the spinner result and the die result, but this time it tells us the probability of seeing that spinner result once we know how the die came up.\n",
    "\n",
    "So, if we plug in (1,Red) we are told $P(Spinner=Red|Die=1)$ and we see that the probability is 1. Knowing that the die is 1 means we can be certain that the spinner is red. Check back to the original table to see why this is so.\n",
    "\n",
    "$$P(Die|Spinner)$$\n",
    "Die result GIVEN spinner result\n",
    "\n",
    "|result|Red|Green|Blue|\n",
    "|-|-|-|-|-|-|-|-|\n",
    "|**1**|.33|.0|.0|\n",
    "|**2**|.0|.33|.5|\n",
    "|**3**|.33|.0|.0|\n",
    "|**4**|.0|.33|.0|\n",
    "|**5**|.33|.0|.5|\n",
    "|**6**|.0|.33|.0|\n",
    "\n",
    "In contrast $P(Die=1|Spinner=Red)$ is just .33. But this makes sense because the spinner being red clues us in to the die being odd, but we don't know if the die will be 1, 3, or 5.\n",
    "\n",
    "In symbols, we write:\n",
    "$P(X=x|Y=y) = \\frac{P(X=x,Y=y)}{P(Y=y)}$. That is, we take the probability of both spinner=Red and Die=1 and divide it by the thing that DID happen (i.e the stuff after the bar).\n",
    "\n",
    "The above formula should make it clear that in order to form the conditional distribution we need to calculate the marginal distribution of the things that have already occured.\n",
    "\n",
    "### Continuous Conditional Distributions\n",
    "Once again, the math is the same with integral replacing sums. But there aren't any of either in this formula, just implicitly in the sum/integral to find the marginal. Hooray!\n",
    "\n",
    "### More variables\n",
    "$$P(X=x|Y=y,Z=z) = $\\frac{P(X=x,Y=y,Z=z)}{P(Y=y,Z=z)}$$\n",
    "$$P(X=x,Y=y|Z=z) = $\\frac{P(X=x,Y=y,Z=z)}{P(Z=z)}$$\n",
    "\n",
    "In both these formulas the rule is simple: take the joint and divide by the marginal of what's behind the bar (i.e. the facts that are given / the things that have already happened). The idea is that those things have already happened, and we're re-normalizing our probability space to just those events.\n",
    "\n",
    "### Manipulating Bars\n",
    "The above formulae give us a very important way of re-writing joint distributions:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(X=x,Y=y,Z=z) &= P(X=x|Y=y,Z=z)P(Y=y|Z=z)P(Y=y,Z=z)\\\\\n",
    "P(X=x,Y=y,Z=z) &= P(X=x|Y=y,Z=z)P(Y=y,Z=z)\\\\\n",
    "P(X=x,Y=y,Z=z) &=  P(X=x,Y=y|Z=z)P(Z=z)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Examining those in reverse, we can also establish the rule that if you multiply a conditional by a marginal that's behind the bar, those terms get to move in front of the bar:\n",
    "$$P(X=x,Z=z|Y=y) = P(X=x|Y=y,Z=z)P(Z=z)$$\n",
    "Similarly, you can put things behind bars if you multiply by the marginal\n",
    "\n",
    "## Summary\n",
    "Conditional probabilites are a lot of work, but great once you have them. \n",
    "\n",
    "We saw in the rules of probability that they show up anytime we want to find the probability of \"A and B\" when A and B are non-independent (so, most of the time), and even more in multi-variable contexts where $P(Spinner=Red,Die=2)$ is literally read as \"The spinner is three *and* the die is 2\".\n",
    "\n",
    "Further, conditional probabilites are useful in prediction problems where we might want to say \"Never mind the probability that this person is 170cm and 45 kilos; it happened, what now?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another View\n",
    "\n",
    "The diagram  below taken from Bishop may be used to illustrate the concepts of conditionals and marginals. Consider two random variables, $X$, which takes the values ${x_i}$ where\n",
    "$i = 1,...,M$, and $Y$, which takes the values ${y_j}$ where $j = 1,...,L$. The number of instances for which $X = x_i$ and $Y = y_j$ is $n_{ij}$. The number of points in column i where $X=x_i$ is $c_i$, and for the row where $Y = y_j$ is $r_j$.\n",
    "\n",
    "\n",
    "![m:bishopprob](./images/bishop-prob.png)\n",
    "\n",
    "Then the **joint probability** of having  $p(X = x_i, Y= y_j)$ is in the asymptotic limit of large numbers in the frequency sense of probability $n_{ij}/N$ where is the total number of instances. The $X$ **marginal**,  $p(X=x_i)$ can be obtained by summing instances in all the cells in the  i'th column:\n",
    "\n",
    "$$p(X=x_i) = \\sum_j p(X=x_i, Y=y_j)$$\n",
    "\n",
    "Lets consider next only those instances for which  $X=x_i$. This means that we are limiting our analysis to the ith row. Then, we write the **conditional probability** of $Y = y_j$ given $X = x_i$ as $p(Y = y_j \\mid X = x_i)$. This is the asymptotic fraction of these instances where $Y = y_j$ and is obtained by dividing the instances in the cell by those in the comumn as \n",
    "\n",
    "$$p(Y = y_j \\mid X = x_i) = \\frac{n_{ij}}{c_i}.$$\n",
    "\n",
    "A little algebraic rearrangement gives:\n",
    "\n",
    "$$p(Y = y_j \\mid X = x_i) = \\frac{n_{ij}}{c_i} = \\frac{n_{ij}}{N} / \\frac{c_i}{N},$$\n",
    "\n",
    "or:\n",
    "\n",
    "$$p(Y = y_j \\mid X = x_i) \\times p(X=x_i) =  p(X=x_i, Y=y_j).$$\n",
    "\n",
    "This is the product rule of probability with conditionals involved.\n",
    "\n",
    "Let us simplify the notation by dropping the $X=$ and $Y=$.\n",
    "\n",
    "Then we can write the marginal probability of x as a sum over the joint distribution of x and y where we sum over all possibilities of y,\n",
    "\n",
    "$$p(x) = \\sum_y p(x,y) $$.\n",
    "\n",
    "We can rewrite a joint distribution as a product of a conditional and marginal probability,\n",
    "\n",
    "$$ p(x,y) = p(y\\mid x) p(x) $$\n",
    "\n",
    "The product rule is applied repeatedly to give expressions for the joint\n",
    "probability involving more than two variables. For example, the joint distribution over three\n",
    "variables can be factorized into a product of conditional probabilities:\n",
    "\n",
    "$$ p(x,y,z) = p(x|y,z) \\, p(y,z) = p(x |y,z) \\, p(y|z) p(z) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
