{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\newcommand{\\isum}{\\sum_{i}}$$\n",
    "$$\\newcommand{\\zsum}{\\sum_{k=1}^{K}}$$\n",
    "$$\\newcommand{\\zsumi}{\\sum_{\\{z_i\\}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The EM algorithm with indices laid out\n",
    "\n",
    "I often find it confusing as to what the indices are actually doing if I dont write them out explicitly. So lets visit the EM derivation once more, focussing on mixtures, and explicitly writing out indices. The derivation does not need mixtures, but I find it helpful to imagine that we are fitting such a model.\n",
    "\n",
    "Suppose we have an estimation problem in which we have data consising of $m$ independent examples $\\{x_1,\\ldots,x_m\\}$ . \n",
    "The goal is to fit the parameters of the model, where the log-likelihood is given by \n",
    "\\begin{eqnarray}\n",
    "\\ell(x  \\vert  \\theta)&=& \\log \\prod_{i=1}^{m} p(x_i \\vert  \\theta) =   \\sum_{i=1}^{m} \\log \\,p(x_i \\vert  \\theta)  \\\\ \n",
    "   &=& \\sum_{i=1}^{m} \\log \\zsumi \\,p(x_i,z \\vert  \\theta)  \\\\ \n",
    "\\end{eqnarray}\n",
    "\n",
    "where the $z$ are the latent random variables. If $z$ were observed then the maximum likelihood estimation would be easy. \n",
    "\n",
    "Indeed then, let us start with the full data log-likelihood, \n",
    "\n",
    "$$\\ell(x, z  \\vert  \\theta) = \\sum_{i=1}^{m}  \\log \\,p(x_i, z_i  \\vert  \\theta),$$\n",
    "\n",
    " which is the log-likelihood we'd calculate if we knew all the $z_i$. But we do not know thse, so lets assume the $\\{z_i\\}$ have some normalized distribution $q(z)$, and calculate the expected value of the full data log likelihood with respect to this distribution:\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathrm{E_q[\\ell( x,z  \\vert  \\theta)]}  &=& \\sum_i \\zsumi q_{i}(z_i) \\log \\,p(x_i, z_i  \\vert  \\theta)\\\\\n",
    "    &=& \\sum_i \\zsumi q_{i}(z_i) \\log \\,\\frac{p(x_i, z_i  \\vert  \\theta)}{q_{i}(z_i)} +  \\sum_i \\zsumi q_{i}(z_i) \\log \\,q_{i}(z_i)\n",
    "\\end{eqnarray}\n",
    "\n",
    "The second term only involves $q$ and is independent of $\\theta$. Looking only at the first term inside the i-summation:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathcal{L}(i, q, \\theta) &=&  \\zsumi q_{i}(z_i) \\log \\,\\frac{p(x_i, z_i  \\vert  \\theta)}{q_{i}(z_i)} \\\\\n",
    "&=& \\zsumi  q_i(z_i) \\left( \\log \\frac{p(z_i \\vert  x_i,  \\theta)}{ q_i(z_i)} + \\log p(x_i  \\vert  \\theta)\\right)\n",
    "\\end{eqnarray}\n",
    "\n",
    "we can see that, since $\\zsumi q_i(z_i) = 1$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathcal{L}(i, q, \\theta) &=& \\zsumi  q_i(z_i) \\left( \\log \\frac{p(z_i \\vert  x_i,  \\theta)}{ q_i(z_i)} + \\log p(x_i  \\vert  \\theta)\\right)\\\\\n",
    "    &=& -\\mathrm{KL}\\left(q_i  \\vert  \\vert  p_i \\right) + \\log p(x_i  \\vert  \\theta)\\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "where $\\mathrm{KL}$ is the Kullback-Leibler divergence between $q(x)$ and the hidden variable posterior distribution $p(z \\vert x,\\theta)$ at the poin $i$.\n",
    "\n",
    "Since the sum over the data-points of the second term is just the log-likelihood we desire, it can then can be written as:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\ell(x  \\vert  \\theta) &=& \\sum_i \\left(\\mathcal{L}(i, q, \\theta) +\\mathrm{KL}\\left(q_i   \\vert  \\vert  p_i \\right)\\right)\\\\\n",
    "&=& \\mathcal{L}(q, \\theta) + \\mathrm{KL}\\left(q  \\vert  \\vert  p \\right)\n",
    "\\end{eqnarray}\n",
    "\n",
    "where we are defining:\n",
    "\n",
    "$$\\mathrm{KL}(q  \\vert  \\vert  p) = \\sum_i \\mathrm{KL}\\left(q_i   \\vert  \\vert  p_i \\right)$$\n",
    "\n",
    "as the sum of the KL-divergence at each data point, and $\\mathcal{L}(q, \\theta)$ as the sum of $\\mathcal{L}$ at each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
