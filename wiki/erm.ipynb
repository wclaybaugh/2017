{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERM \n",
    "\n",
    "The Empirical Risk Minimization (ERM) approach corresponds to estimating the true distribution by the empirical distribution. In this case the Risk R is simply the average of the losses at the individual training points:\n",
    "\n",
    "$$ R(g) = \\frac{1}{N} \\sum_i l(g(x_i), y_i) .$$\n",
    "\n",
    "(Diagrams like the one below are chopped out from http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/240415.pdf . Reading Chapter 13 as a survey of machine learning is especially recommended)\n",
    "\n",
    "![ERM learning](https://dl.dropboxusercontent.com/u/75194/erm.png)\n",
    "\n",
    "The optimal decision in the training set is obviously the value of $y$ at the training point, but we are left with undefined action $g$ outside the training set. We thus pick some parametric model $g(x;\\theta)$. Now we can minimize the empirical risk with respect to $\\theta$ to get $\\theta_{opt}$ and use this to make predictions/actions using $g$. Because such an approach can lead to overfitting as we have seen before, we typically add a regularization term with co-efficient $\\lambda$ whose value is found by validation.\n",
    "\n",
    "Notice that in all of this any talk of density estimation has gone away, and we are just minimizine the averaged loss over the training set plus regularization, the so-called Structural Risk minimization approach of Vapnik, whose motto is (paraphrased): **Never solve a more difficult problem (density estimation) while solving a difficut one (learning)**. The function $g$ is then sometimes called a discriminant function, and we are choosing it based on *minimal risk, which is the quantity we are ultimately interested in*. \n",
    "\n",
    "![](http://yann.lecun.com/ex/images/allyourbayes.jpg)\n",
    "\n",
    "But there are drawbacks. It seems crazy to assume that the empirical distribution is a good distribution, especially for small data. A more reasonable assumption for the distribution could take into account likely x,y that could arise. If the loss changes, as it might over time, say in a financial application, then we would need to retrain $g$. There is no way to associate a confidence in this framework, as it wont give you probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes\n",
    "\n",
    "![Bayesian learning](images/bayesrisk.png)\n",
    "\n",
    "The alternative is to first do density estimation. We estimate $p(x,y)$ (or $p(x,c)$) from the training data. (Note that this can be thought of as ERM on risk $-log(p)$). (In the \"Learning Models\" lab we said that another way to think about a noisy $y$ is to imagine that our data $\\dat$ was generated from  a joint probability distribution $p(x,y)$ rather than some well given function$y=f(x)$. That is, given the x, there is a distribution of possible observed y values, instead of a single y value)\n",
    "\n",
    "The joint distribution can be constructed in two ways: generative or discriminative. The **discriminative** or **non-generative** approach gives us:\n",
    "\n",
    "$$p(x,c) = p(c|x)p(x)$$\n",
    "\n",
    "whereas the **generative approach** gives us\n",
    "\n",
    "$$p(x,c) = p(x|c) p(c)$$\n",
    "\n",
    "and then bayes theorem can be used to obtain p(c|x).\n",
    "\n",
    "The generative approach corresponds to picking one of the classes with probability p(c) and then getting the density of the features for that class. The discriminative approach models the domain boundary instead. While the data may be distributed in a complex way, the boundary may be easier to model. On the other hand prior information for assymetric situations, conditional independence and other such strategies can only be done in generative models.\n",
    "\n",
    "![Bayesian learning](images/genvsdiscrim.png)\n",
    "\n",
    "In either case we can get the joint distribution. In the discriminative case that leads us to density estimation for $p(x)$. Often we have no use for it so we wont do it, as in logistic regression. But do remember that if we want our classifier to have good results we should be using it on test sets which reflect the proper sampling $p(x)$. And if we dont characterize it we might be better of using a generative model as it is easier to adjust for class priors.\n",
    "\n",
    "The Bayesian decision approach is a clean one, in which first one models the **environment**, independent of the subsequent decision process. If $p(y,x|\\theta)$ is the \"true\" model of the world, this is optimal. But if this **environment model** is poor, the action $g$ could be higly inaccurate since the environment is divorced from prediction. In practice one often includes regularization terms in the environment model to reduce the complexity of the distribution and bring it more in line with decision based hyperparameters, set by validation on an empirical loss. See the cs109 (2013) Naives bayes homework for a good example.\n",
    "\n",
    "By the way, the ERM method is the only **frequentist** method which has a well defined risk. The reason for this is that it dosent depend on both a sample-estimate of the true $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
